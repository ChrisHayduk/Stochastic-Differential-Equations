\documentclass[12pt]{article}
 
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amsthm,amssymb, mathtools}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{fixltx2e}
\usepackage[shortlabels]{enumitem}
\usepackage{mathrsfs}
\allowdisplaybreaks

 
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}

\newcommand{\stcomp}[1]{{#1}^\complement}
 
\newenvironment{theorem}[2][Theorem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{lemma}[2][Lemma]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{exercise}[2][Exercise]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{question}[2][Question]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{corollary}[2][Corollary]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newcommand{\textfrac}[2]{\dfrac{\text{#1}}{\text{#2}}}

\begin{document}

\title{Stochastic Differential Equations: Final Project}

\author{Chris Hayduk}
\date{\today}

\maketitle

\textbf{NOTE: Still need to complete Problems 2, 7, 8, 15, 17, 18, 22, 23c, 29, 30, 31bc}

\begin{problem}{1}
\end{problem}

We know that the probability density of the sum of two independent random variables can be computed by the convolutions of $f_X$ and $f_Y$. Thus, the probability density of $X + Y$, denoted by $f_Z$, is
\begin{align}
f_Z(z) = (f_Y*f_X) = \int_{-\infty}^{\infty}  f_Y(z - x)f_X(x) dx
\end{align}

We know that the normal density function is given by,
\begin{align*}
f(x) = \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2}
\end{align*}

Plugging this density function into equation (1) and letting $\sigma_Z = \sqrt{\sigma_X^2 + \sigma_Y^2}$ yields,
\begin{align*}
f_Z(z) &= \int_{-\infty}^{\infty} \left[\frac{1}{\sigma_Y\sqrt{2\pi}} e^{-\frac{1}{2}\left(\frac{(z-x)-\mu_Y}{\sigma_Y}\right)^2} \frac{1}{\sigma_X\sqrt{2\pi}} e^{-\frac{1}{2}\left(\frac{x-\mu_X}{\sigma_X}\right)^2}\right] dx\\
&= \int_{-\infty}^{\infty} \left[\frac{1}{\sigma_Y\sigma_X(2\pi)} e^{-\frac{1}{2}\left[\left(\frac{z-x-\mu_Y}{\sigma_Y}\right)^2 + \left(\frac{x-\mu_X}{\sigma_X}\right)^2 \right]} \right] dx\\
&= \int_{-\infty}^{\infty} \left[\frac{1}{\sigma_Y\sigma_X(2\pi)} e^{-\frac{\sigma_X^2(z-x-\mu_Y)^2 + \sigma_Y^2(x-\mu_X)^2}{2 \sigma_Y^2 \sigma_X^2}} \right] dx\\
&= \int_{-\infty}^\infty \left[\frac{1}{(2\pi)\sigma_X\sigma_Y} e^{
      -\frac
         {
            x^2(\sigma_X^2 + \sigma_Y^2) - 
            2x(\sigma_X^2(z - \mu_Y) + \sigma_Y^2\mu_X) +
            \sigma_X^2(z^2 + \mu_Y^2 - 2z\mu_Y) + \sigma_Y^2\mu_X^2
         }
         {2\sigma_Y^2\sigma_X^2}}\right] dx\\
&= \int_{-\infty}^\infty
   \left[ \frac{1}{\sqrt{2\pi}\sigma_Z}
    e^{-\frac
         {
            \sigma_Z^2\left(\sigma_X^2(z - \mu_Y)^2 + \sigma_Y^2\mu_X^2\right) -
            \left(\sigma_X^2(z - \mu_Y) + \sigma_Y^2\mu_X\right)^2
         }
         {2\sigma_Z^2\left(\sigma_X\sigma_Y\right)^2}}
   \frac{1}{\sqrt{2\pi}\frac{\sigma_X\sigma_Y}{\sigma_Z}}
    e^{-\frac
         {
            \left(x - \frac{\sigma_X^2(z - \mu_Y) + \sigma_Y^2\mu_X}{\sigma_Z^2}\right)^2
         }
         {2\left(\frac{\sigma_X\sigma_Y}{\sigma_Z}\right)^2}} \right]
    dx \\
&= \frac{1}{\sqrt{2\pi}\sigma_Z}
   e^{- { (z-(\mu_X+\mu_Y))^2 \over 2\sigma_Z^2 }}
   \int_{-\infty}^{\infty} \left[
   \frac{1}{\sqrt{2\pi}\frac{\sigma_X\sigma_Y}{\sigma_Z}}
   e^{- \frac{\left(x-\frac{\sigma_X^2(z-\mu_Y)+\sigma_Y^2\mu_X}{\sigma_Z^2}\right)^2}{2\left(\frac{\sigma_X\sigma_Y}{\sigma_Z}\right)^2}} \right]
    dx
\end{align*}

The equation inside the integral symbol represents a valid normal density function for $x$, so we know it integrates to $1$. Thus, the probability density function for $X + Y$ is given by
\begin{align*}
f_Z(z) = \frac{1}{\sqrt{2\pi}\sigma_Z}
   e^{- { (z-(\mu_X+\mu_Y))^2 \over 2\sigma_Z^2 }}
\end{align*}

This is precisely a normal density function with mean $\mu_X + \mu_Y$ and variance $\sigma_Z^2 = (\sqrt{\sigma_X^2 + \sigma_Y^2})^2 = \sigma_X^2 + \sigma_Y^2$.\\

Hence, we have shown that, given $X \sim N(\mu_X, \sigma_X^2)$ and $Y \sim N(\mu_Y, \sigma_Y^2)$, $X + Y$ is distributed as $N(\mu_X + \mu_Y, \sigma_X^2 + \sigma_Y^2)$.

\begin{problem}{2}
\end{problem}

Let $Z = Y + 1$. We'll begin by finding the probability density function for $Z$. Since $Y$ is a standard uniform random variable, we know that
\begin{align*}
f(y) = \begin{cases} 
      1 & 0 \leq y \leq 1 \\
      0 & \text{elsewhere} 
   \end{cases}
\end{align*}

Thus, the distribution function approach yields,
\begin{align*}
F_Z(z) = P(Z \leq z) = P(Y + 1 \leq z) &= P(Y \leq z - 1)\\
&= \int_{-\infty}^{z-1} f(y)dy \\
&= \int_0^{z-1} 1 \, dy\\
&= z-1
\end{align*}

So, since as $Y$ ranges from 0 to 1, $Z$ ranges from 1 to 2, we have
\begin{align*}
F_Z(z) = \begin{cases} 
      0 & z < 1 \\
      z-1 & 1 \leq z \leq 2\\
      1 & \text{elsewhere} 
   \end{cases}
\end{align*}

And the density function for $Z$ is
\begin{align*}
f_Z(z) = \frac{dF_Z(z)}{dz} = \begin{cases} 
      1 & 1 \leq z \leq 2\\
      0 & \text{elsewhere} 
   \end{cases}
\end{align*}

Now we can simplify the original function to be $U = X/Z$, where we now know the probability density function for both random variables ($X \sim \text{Unif}(0,1)$ and $Z \sim \text{Unif}(1, 2)$).\\

So the distribution function is given by,
\begin{align*}
F_U(u) = P(U \leq u) = P(X/Z \leq u) &= P(X \leq uZ)\\
&= \int_{-\infty}^{uz} f(x)dx \\
&= \int_0^{uz} 1 \, dx\\
&= uz
\end{align*}

FINISH THIS LATER.

\begin{problem}{3}
\end{problem}

Let $Y$ be a standard normal variable. Then the moment-generating function of $Y$ is given by,
\begin{align*}
m(t) &= E(e^{tY})\\
&= \int_{-\infty}^{\infty} e^{ty}f(y) \, dy\\
\end{align*}

Since $Y$ is a standard normal variable, we know that $\mu = 0$ and $\sigma = 1$. Thus, we have 
\begin{align*}
f(y) = \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}y^2}
\end{align*}

Plugging this into the above equation yields,
\begin{align*}
m(t) &= \int_{-\infty}^{\infty} e^{ty} \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}y^2} \, dy\\
&= \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} e^{-\frac{y^2}{2} + ty} \, dy\\
&= \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}(y - t)^2} e^{\frac{1}{2} t^2} \, dy\\
&= e^{\frac{1}{2} t^2} \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}(y - t)^2} \, dy
\end{align*}

We can see that the integral is precisely the integral for a normal random variable with $\mu = y - t$ and $\sigma = 1$. Thus, it integrates to $1$, yielding,
\begin{align*}
m(t) = e^{\frac{1}{2} t^2}
\end{align*}

\begin{problem}{4}
\end{problem}

Suppose $g(x)$ is a monotone increasing function and $X$ is a random variable with the probability density function $f_X$.\\

Let $U = g(X)$ where $X$ has the above density function. Since $g(x)$ is an increasing function of $x$, then $g^{-1}(u)$ is an increasing function of $u$. Thus,
\begin{align*}
P(U \leq u) &= P[g(X) \leq u]\\
&= P\{g^{-1}[g(X)] \leq g^{-1}(u)\}\\
&= P[X \leq g^{-1}(u)]
\end{align*}

The above sequence of equalities implies that,
\begin{align*}
F_U(u) = F_X[g^{-1}(u)]
\end{align*}

When we differentiate with respect to $u$, we get,
\begin{align*}
f_U(u) = \frac{dF_X[g^{-1}(u)]}{du} = f_X(g^{-1}(u))\frac{d[g^{-1}(u)]}{du}
\end{align*}

\begin{problem}{5}
\end{problem}

We know that the moment-generating function of $X$ is $\phi(t)$. Thus, 
\begin{align*}
\phi(t) &= E(e^{tX})\\
&= \int_{-\infty}^{\infty} e^{tx} f(x) \, dx\\
&= \int_{-\infty}^{\infty} \left(1 + tx + \frac{t^2x^2}{2!} + \frac{t^3x^3}{3!} + \cdots \right) f(x) \, dx\\
&= \int_{-\infty}^{\infty} f(x) dx + t \int_{-\infty}^{\infty} xf(x)dx + \frac{t^2}{2!} \int_{-\infty}^{\infty} x^2f(x)dx + \cdots
\end{align*}

Now plugging in $aX + b$ to the above equation yields,
\begin{align*}
E(e^{t(aX+b)}) &= E(e^{taX + tb})\\
&= E[e^{taX}(e^{tb})]
\end{align*}

Since expected value is a linear operator and $e^{tb}$ is a constant, we can pull it out of the expectation operator,
\begin{align*}
E[e^{taX}(e^{tb})] &= e^{tb} E(e^{taX})\\
&= e^{tb} \int_{-\infty}^{\infty} e^{tax} f(ax) \, dx
\end{align*}

We can see from the above equation that this is equal to $e^{tb}\phi(ta)$.\\

Thus, the moment-generating function for $aX + b$ with constants $a \neq 0$ and $b$ is $e^{tb}\phi(ta)$.

\begin{problem}{6}
\end{problem}

We know by Theorem 2.11.1 that the distribution of the sum of two random variables $X$ and $Y$ is given by the convolution of their densities, $f_X$ and $f_Y$.\\

Thus, with $Z = X + Y$, we have,
\begin{align*}
f_Z(z) = (f*g)(z) = \int_{-\infty}^{\infty} f_X(z-y)f_Y(y) dy
\end{align*}

The mean of this distribution is,
\begin{align*}
E(Z) = \int_{-\infty}^{\infty} zf_X(z-y)f_Y(y) dy
\end{align*}

where $z = x+y$. Thus, we have,
\begin{align*}
E(Z) &= \int_{-\infty}^{\infty} zf_Z(z) dz\\
&= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \left[(x+y)f_X((x+y)-y)f_Y(y)\right] dxdy\\
&= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \left[(x+y)f_X(x)f_Y(y)\right] dxdy\\
&= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \left[(xf_X(x) + yf_X(x))f_Y(y)\right] dxdy\\
&= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \left[xf_X(x)f_Y(y) + yf_X(x)f_Y(y)\right] dxdy\\
&= \int_{-\infty}^{\infty} \left[\mu_Xf_Y(y) + yf_Y(y)\right]dy\\
&= \mu_X + \mu_Y
\end{align*}

In the above equations, we are using the properties that, for any random variable $X$ with density function $f(x)$ and mean $\mu$, we have that $\int_{-\infty}^{\infty} f(x) = 1$ and  $\int_{-\infty}^{\infty} xf(x) = \mu$.\\

Now for the variance, we know that for a given random variable $X$, $\text{Var}(X) = E[X^2] - E[X]^2$. Plugging in $Z$ to this formula yields,
\begin{align*}
\text{Var}(Z) = \sigma_Z^2 &= E[Z^2] - E[Z]^2\\
&= \int_{-\infty}^{\infty} z^2f_Z(z) dz + (\mu_X + \mu_Y)^2\\
&= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \left[(x+y)^2f_X((x+y)-y)f_Y(y)\right] dxdy - (\mu_X + \mu_Y)^2\\
&= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \left[(x+y)^2f_X(x)f_Y(y)\right] dxdy - (\mu_X + \mu_Y)^2\\
&= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \left[(x^2 + 2xy + y^2)f_X(x)f_Y(y)\right] dxdy - (\mu_X + \mu_Y)^2\\
&= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \left[x^2f_X(x)f_Y(y) + 2xyf_X(x)f_Y(y) + y^2f_X(x)f_Y(y)\right] dxdy - (\mu_X + \mu_Y)^2\\
&= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} x^2f_X(x)f_Y(y) dxdy - \mu_X + \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} 2xyf_X(x)f_Y(y) dxdy\; + \\&\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} y^2f_X(x)f_Y(y) dxdy - \mu_Y\\
&= \left[\int_{-\infty}^{\infty} x^2f_X(x)dx - \mu_X\right] + 2\mu_X\mu_Y + \left[\int_{-\infty}^{\infty} y^2f_Y(y)dy - \mu_Y\right]\\
&= \sigma_X^2 + 2\mu_X\mu_Y + \sigma_Y^2
\end{align*}

Thus, we have $\text{Var}(Z) = \sigma_X^2 + 2\mu_X\mu_Y + \sigma_Y^2$.

\begin{problem}{7}
\end{problem}

\noindent The definition of the Central Limit Theorem is as follows:\\
\\
Let $Y_1, Y_2, ..., Y_n$ be independent and identically distributed random variables with $E(Y_i) = \mu$ and $V(Y_i) = \sigma^2 < \infty$. Define
\begin{align*}
U_n = \frac{\Sigma_{i=1}^n Y_i - n\mu}{\sigma\sqrt{n}} = \frac{\overline{Y} - \mu}{\sigma/\sqrt{n}}
\end{align*}

\noindent where $\overline{Y} = \frac{1}{n} \Sigma_{i=1}^n Y_i$.\\

\noindent Then the distribution function of $U_n$ converges to the standard normal distribution function as $n \to \infty$. That is,
\begin{align*}
\lim_{n \to \infty} P(U_n \leq u) = \int_{-\infty}^{u} \frac{1}{\sqrt{2\pi}}e^{-t^2/2}dt
\end{align*}

\noindent for all $u$.\\

\noindent Let $f_n(x)$ be the probability mass function for a binomial random variable with $n$ trials and success probability $p$.

FINISH THIS LATER.

\begin{problem}{8}
\end{problem}

Let $X$ and $Y$ be two independent, continuous random variables described by probability density functions $f_X$ and $f_Y$. Also let $Z = XY$. We'll begin by finding the cumulative distribution function for $Z$. This yields,
\begin{align*}
F_Z(z) &= P(Z \leq z)\\
&= P(XY \leq z)\\
&= P(XY \leq z, X \geq 0) + P(XY \leq z, X \leq 0)\\
&= P(Y \leq z/X, X \geq 0) + P(Y \geq z/X, X \leq 0)\\
&= \int_{0}^{\infty} f_X(x) \int_{-\infty}^{z/x} f_Y(y) dy dx + \int_{-\infty}^0 f_X(x) \int_{z/X}^{\infty} f_Y(y) dy dx
\end{align*}

Now in order to find the probability density function for $Z$, we need to differentiate with respect to $z$ on both sides of the above equation.
\begin{align*}
f_Z(z) &= \frac{d}{dz}F_z(z)\\
&= \frac{d}{dz}\left[\int_{0}^{\infty} f_X(x) \int_{-\infty}^{z/x} f_Y(y) dy dx + \int_{-\infty}^0 f_X(x) \int_{z/X}^{\infty} f_Y(y) dy dx\right]\\
&= \int_{0}^{\infty} f_X(x) \left[f_Y(z/x)\left(\frac{1}{x}\right) - f(-\infty)\right] dx + \int_{-\infty}^{0} f_X(x) \left[f_Y(\infty) - f_Y(z/x)\left(\frac{1}{x}\right)\right] dx
\end{align*}

We obtained the above equation using the fundamental theorem of calculus and the chain rule. Now, we will use the fact that if $f(x)$ is a distribution function, as $x \to \infty$, $f(x) \to 0$. The same holds true as $x \to -\infty$. This yields,
\begin{align*}
f_Z(z) &= \int_{0}^{\infty} f_X(x) \left[f_Y(z/x)\left(\frac{1}{x}\right) - f(-\infty)\right] dx + \int_{-\infty}^{0} f_X(x) \left[f_Y(\infty) - f_Y(z/x)\left(\frac{1}{x}\right)\right] dx\\
&= \int_{0}^{\infty} f_X(x)f_Y(z/x)\frac{1}{x} dx + \int_{-\infty}^{0} f_X(x)(-f_Y(z/x))\frac{1}{x} dx\\
&= \int_{0}^{\infty} f_X(x)f_Y(z/x)\frac{1}{x} dx - \int_{-\infty}^{0} f_X(x)f_Y(z/x)\frac{1}{x} dx\\
&= \int_{0}^{\infty} f_X(x)f_Y(z/x)\frac{1}{|x|} dx + \int_{-\infty}^{0} f_X(x)f_Y(z/x)\frac{1}{|x|} dx\\
&= \int_{-\infty}^{\infty} f_X(x)f_Y(z/x)\frac{1}{|x|} dx
\end{align*}

Now we need to find the mean of $Z$ where $Z = XY$,
\begin{align*}
E[Z] &= \int_{-\infty}^{\infty} zf_Z(z)dz\\
&= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} (xy)f_X(x)f_Y(y)\frac{1}{|x|} dxdy\\
&= \mu_Y \int_{-\infty}^{\infty} \frac{x}{|x|}f_X(x)dx\\
&= \mu_Y \left[\int_{-\infty}^0 - f_X(x) dx + \int_{0}^{\infty} f_X(x) dx\right]\\
&= \mu_Y \left[-\int_{-\infty}^0 f_X(x) dx + \int_{0}^{\infty} f_X(x) dx\right]
\end{align*}

FINISH THIS LATER.

\begin{problem}{9}
\end{problem}

We have that,
\begin{align*}
t_j - t_{j-1} &= (j/N)(b-a) + a - [(j-1/N)(b-a) + a]\\
&= (jb - ja)/N + a - (jb - ja - b + a)/N - a\\
&= (jb - ja - jb + ja + b - a)/N\\
&= (b - a)/N
\end{align*}

In addition, since the function $f$ is bounded on $[a,b]$, we know that $\exists M \in \mathbb{N}$ such that $M \geq |f|$. These two facts yield the following,
\begin{align*}
\lim_{N \to \infty} \Sigma^N_{j=1} f(t_{j-1})(t_j - t_{j-1})^p &= \lim_{N \to \infty} \Sigma^N_{j=1} f(t_{j-1})((b - a)/N)^p\\
&\leq \lim_{N \to \infty} \Sigma^N_{j=1} M((b - a)/N)^p\\
&= \lim_{N \to \infty} MN((b - a)/N)^p\\
&= M \lim_{N \to \infty} N((b - a)/N)^p\\
&= M(b-a)^p \lim_{N \to \infty} N/N^p\\
&= M(b-a)^p \lim_{N \to \infty} N^{1-p}
\end{align*}

Since $1-p < 0$, we have that $\lim_{N \to \infty} N^{1-p} = 0$. Thus, this gives us,
\begin{align*}
\lim_{N \to \infty} \Sigma^N_{j=1} f(t_{j-1})(t_j - t_{j-1})^p &\leq M(b-a)^p \lim_{N \to \infty} N^{1-p}\\
&= [M(b-a)]0 = 0
\end{align*}

Now, if we use $-M$ for the lower bound, we get,
\begin{align*}
\lim_{N \to \infty} \Sigma^N_{j=1} f(t_{j-1})(t_j - t_{j-1})^p &\geq -M(b-a)^p \lim_{N \to \infty} N^{1-p}\\
&= [-M(b-a)]0 = 0
\end{align*}

Since we have shown that
\begin{align*}
0 \leq \lim_{N \to \infty} \Sigma^N_{j=1} f(t_{j-1})(t_j - t_{j-1})^p \leq 0
\end{align*}

we have that $\lim_{N \to \infty} \Sigma^N_{j=1} f(t_{j-1})(t_j - t_{j-1})^p = 0$.

\begin{problem}{10}
\end{problem}

We know, by Proposition 4.9.1, that for a given stochastic process $X_t$, if $E[X_t] \to k$ a constant and $\text{Var}(X_t) \to 0$ as $t \to \infty$, then $\operatorname{ms-\lim}_{t \to \infty} X_t = k$.\\

Thus, we will need to check the mean and variance of the given function. We will begin with the mean. Using the fact that the function $f$ is a constant in terms of the expected value function, we get
\begin{align*}
E(\lim_{N \to \infty} \Sigma^N_{j=1} f(t_{j-1}) (\Delta_{j-1}^j W)^2) = \lim_{N \to \infty} \Sigma^N_{j=1} f(t_{j-1}) E[(\Delta_{j-1}^j W)^2]
\end{align*}

We know that the expected value of the square of a random variable is its variance, which is $t_j - t_{j-1}$ in this case. Hence,
\begin{align*}
E(\lim_{N \to \infty} \Sigma^N_{j=1} f(t_{j-1}) (\Delta_{j-1}^j W)^2) &= \lim_{N \to \infty} \Sigma^N_{j=1} f(t_{j-1}) E[(\Delta_{j-1}^j W)^2]\\
&= \lim_{N \to \infty} \Sigma^N_{j=1} f(t_{j-1})(t_j - t_{j-1})
\end{align*}

This is the equation for the Riemann integral of $f$ over $[a,b]$. Since we already know $f$ is bounded and defined on a closed interval, as long as $f$ is continuous almost everywhere on $[a,b]$ we assert that the above limit exists and is equal to the integral of $f$. If that is the case, then we have
\begin{align*}
E(\lim_{N \to \infty} \Sigma^N_{j=1} f(t_{j-1}) (\Delta_{j-1}^j W)^2) &= \lim_{N \to \infty} \Sigma^N_{j=1} f(t_{j-1})(t_j - t_{j-1})\\
&= \int_a^b f(t) dt
\end{align*}

Now we need to show that the variance of the function is 0. Using Problem 8 to show that $\text{Var}[(\Delta_{j-1}^j W)^2] = 2(t_j - t_{j-1})^2$, we have
\begin{align*}
\text{Var}(\lim_{N \to \infty} \Sigma^N_{j=1} f(t_{j-1}) (\Delta_{j-1}^j W)^2) &= \lim_{N \to \infty} \Sigma^N_{j=1} (f(t_{j-1}))^2 \text{Var}[(\Delta_{j-1}^j W)^2]\\
&= \lim_{N \to \infty} \Sigma^N_{j=1} (f(t_{j-1}))^2 \cdot 2(t_j - t_{j-1})^2\\
&= 2\lim_{N \to \infty} \Sigma^N_{j=1} (f(t_{j-1}))^2 (t_j - t_{j-1})^2
\end{align*}

Now observe that if $M$ is an upper bound for $|f|$ on $[a,b]$ (as in Problem 9), then $M^2$ is an upper bound for $|f|^2$ on $[a,b]$. This is true because increasing functions preserve inequalities, squaring is an increasing function for $x \geq 0$, and $|f| \geq 0$ for every $x$. In addition, observe that $|f|^2 = f^2$. Thus, we have that $M^2$ is an upper bound for $f^2$ on $[a,b]$.\\

We can now the above and the result from Problem 9 in order to obtain the following,
\begin{align*}
\text{Var}(\lim_{N \to \infty} \Sigma^N_{j=1} f(t_{j-1}) (\Delta_{j-1}^j W)^2) &= 2\lim_{N \to \infty} \Sigma^N_{j=1} (f(t_{j-1}))^2 (t_j - t_{j-1})^2\\
&\leq 2M^2 \lim_{N \to \infty} \Sigma^N_{j=1} (t_j - t_{j-1})^2\\
&= 2M^2(0) = 0
\end{align*}

In addition, since we know that variance is always $\geq 0$, we have that,
\begin{align*}
\text{Var}(\lim_{N \to \infty} \Sigma^N_{j=1} f(t_{j-1}) (\Delta_{j-1}^j W)^2) = 0
\end{align*}

Thus, we have shown that this function satisfies Proposition 4.9.1 and can state that $\operatorname{ms-\lim}_{t \to \infty} [\lim_{N \to \infty} \Sigma^N_{j=1} f(t_{j-1}) (\Delta_{j-1}^j W)^2] = \int_a^b f(t) dt$.

\begin{problem}{11}
\end{problem}

We need to show that the mean and variance of this function are $0$. We'll begin with the mean,
\begin{align*}
E[\lim_{N \to \infty} \Sigma^N_{j=1} f(t_{j-1})(t_j - t_{j-1})^p(\Delta_{j-1}^j W)^q] &= \lim_{N \to \infty} \Sigma^N_{j=1} f(t_{j-1})(t_j - t_{j-1})^p E[(\Delta_{j-1}^j W)^q]
\end{align*}

If $q$ is odd, then $E[(\Delta_{j-1}^j W)^q] = 0$. If $q$ is even, then $E[(\Delta_{j-1}^j W)^q] = \sigma^q(q-1)!! = (\sqrt{t_j - t_{j-1}})^p(p-1)!!$ where $!!$ is the double factorial. Since the expected value of the whole function is trivially $0$ if $q$ is odd, we will only consider the situation where $q$ is even,
\begin{align*}
\lim_{N \to \infty} \Sigma^N_{j=1} f(t_{j-1})(t_j - t_{j-1})^p E[(\Delta_{j-1}^j W)^q] &= \lim_{N \to \infty} \Sigma^N_{j=1} f(t_{j-1})(t_j - t_{j-1})^p (\sqrt{t_j - t_{j-1}})^q(q-1)!!\\
&= \lim_{N \to \infty} \Sigma^N_{j=1} f(t_{j-1})(t_j - t_{j-1})^p (t_j - t_{j-1})^{q/2}(q-1)!!\\
&= (q-1)!! \lim_{N \to \infty} \Sigma^N_{j=1} f(t_{j-1})(t_j - t_{j-1})^{p + q/2}
\end{align*}

By the constraints in the problem, we know that $p + q/2 > 1$. Hence, we can apply the result of Problem 9. Hence, the limit is equal to 0,
\begin{align*}
E[\lim_{N \to \infty} \Sigma^N_{j=1} f(t_{j-1})(t_j - t_{j-1})^p(\Delta_{j-1}^j W)^q] &= (q-1)!! \lim_{N \to \infty} \Sigma^N_{j=1} f(t_{j-1})(t_j - t_{j-1})^{p + q/2}\\
&= (q-1)!! (0)\\
&= 0
\end{align*}

Now we must show that the variance is equal to $0$,
\begin{align*}
\text{Var}[\lim_{N \to \infty} \Sigma^N_{j=1} f(t_{j-1})(t_j - t_{j-1})^p(\Delta_{j-1}^j W)^q] &= \lim_{N \to \infty} \Sigma^N_{j=1} (f(t_{j-1}))^2 (t_j - t_{j-1})^{2p} \cdot \text{Var}[(\Delta_{j-1}^j W)^q]\\
&= \lim_{N \to \infty} \Sigma^N_{j=1} (f(t_{j-1}))^2 (t_j - t_{j-1})^{2p} \cdot E[(\Delta_{j-1}^j W)^{2q}]\\
&= \lim_{N \to \infty} \Sigma^N_{j=1} (f(t_{j-1}))^2 (t_j - t_{j-1})^{2p} \cdot E[(\Delta_{j-1}^j W)^{2q}]\\
&= \lim_{N \to \infty} \Sigma^N_{j=1} (f(t_{j-1}))^2 (t_j - t_{j-1})^{2p} \cdot (t_j - t_{j-1})^{q}(2q-1)\\
&= (2q-1) \lim_{N \to \infty} \Sigma^N_{j=1} (f(t_{j-1}))^2 (t_j - t_{j-1})^{2p+q} 
\end{align*}

By the same reasoning as in Problem 10, this limit is equal to 0. Thus,
\begin{align*}
\text{Var}[\lim_{N \to \infty} \Sigma^N_{j=1} f(t_{j-1})(t_j - t_{j-1})^p(\Delta_{j-1}^j W)^q] &= (2q-1) \lim_{N \to \infty} \Sigma^N_{j=1} (f(t_{j-1}))^2 (t_j - t_{j-1})^{2p+q}\\
&= 0
\end{align*}

Since the mean is equal to a constant (0 in this case) and the variance is equal to 0, we can say that the mean-squared limit of this function is 0.

\begin{problem}{12}
\end{problem}

We have that,
\begin{align*}
\Sigma_{k=m}^n f_k(g_{k+1} - g_k) &= f_m(g_{m+1} - g_m) + f_{m+1}(g_{m+2} - g_{m+1}) + f_{m+2}(g_{m+3} - g_{m+2}) + \cdots\\
&= f_m(g_m+1) - f_m(g_m) + f_{m+1}(g_{m+2}) - f_{m+1}(g_{m+1}) + \cdots\\
&= - f_m(g_m) + g_{m+1}(f_m - f_{m+1}) + g_{m+2}(f_{m+1} - f_{m+2}) + \cdots + f_n(g_{n+1})\\
&= f_n(g_{n+1}) - f_m(g_m) - \left[g_{m+1}(f_m - f_{m+1}) + g_{m+2}(f_{m+1} - f_{m+2}) + \cdots + g_{n}(f_{n-1} - f_{n})\right]\\
&= f_n(g_{n+1}) - f_m(g_m) - \Sigma_{k = m+1}^{n} g_k (f_{k-1} - f_k)
\end{align*}

\begin{problem}{13}
\end{problem}

We have $F_t = W_t$ with the interval of interest being $[0 , T]$. The partial sums of the integral $\int_0^T W_tdW_t$ are given by
\begin{align*}
S_n &= \Sigma_{i=0}^{n-1} F_{t_i}(W_{t_{i+1}} - W_{t_i})\\
&= \Sigma_{i=0}^{n-1} W_{t_i} (W_{t_{i+1}} - W_{t_i})
\end{align*}

We also have that
\begin{align*}
W_{t_i}(W_{t_{i+1}} - W_{t_i}) = \frac{1}{2}W^2_{t_{i+1}} - \frac{1}{2}W^2_{t_i} - \frac{1}{2}(W_{t_{i+1}} - W_{t_i})^2
\end{align*}

Plugging this into the partial sums formula yields,
\begin{align*}
S_n &= \frac{1}{2} \Sigma_{i=0}^{n-1} W^2_{t_{i+1}} - \frac{1}{2} \Sigma_{i=0}^{n-1} W^2_{t_i} - \frac{1}{2} \Sigma_{i=0}^{n-1} (W_{t_{i+1}} - W_{t_i})^2\\
&= \frac{1}{2}W^2_{t_n} - \frac{1}{2} \Sigma_{i=0}^{n-1} (W_{t_{i+1}} - W_{t_i})^2
\end{align*}

If we let $t_n = T$, we get
\begin{align*}
S_n = \frac{1}{2}W^2_T - \frac{1}{2} \Sigma_{i=0}^{n-1}(W_{t_{i+1}} - W_{t_i})^2
\end{align*}

Using Proposition 4.11.6 and the fact that $\frac{1}{2}W^2_T$ does not depend on $n$, we have
\begin{align*}
\operatorname{ms-\lim}_{n \to \infty} S_n &= \frac{1}{2}W^2_T - \operatorname{ms-\lim}_{n \to \infty} \frac{1}{2} \Sigma_{i=0}^{n-1}(W_{t_{i+1}} - W_{t_i})^2\\
&= \frac{1}{2}W^2_T - \frac{1}{2}T
\end{align*}

Since the Ito integral is defined as the mean-squared limit of the partial sums $S_n$, we have that
\begin{align*}
\int_0^T W_tdW_t &= \operatorname{ms-\lim}_{n \to \infty} S_n\\
&= \frac{1}{2}W^2_T - \frac{1}{2}T
\end{align*}
as required.

\begin{problem}{14}
\end{problem}

We are given the Ito integral,
\begin{align*}
\int_0^T f(t)dW_t
\end{align*}

where $f(t)$ is an arbitrary bounded and continuous function. Each increment is given by,
\begin{align*}
f(t_i)(W_{t_{i+1}} - W_{t_i})
\end{align*}

So the partial sums are given by
\begin{align*}
S_n = \Sigma_{i=0}^{n-1} f(t_i)(W_{t_{i+1}} - W_{t_i})
\end{align*}

We need to find the mean-squared limit of these partial sums. We'll start by finding the expected value,
\begin{align*}
E[\Sigma_{i=0}^{n-1} f(t_i)(W_{t_{i+1}} - W_{t_i})] &= \Sigma_{i=0}^{n-1} f(t_i)E[(W_{t_{i+1}} - W_{t_i})]\\
&= \Sigma_{i=0}^{n-1} f(t_i)(0) = 0
\end{align*}

Thus, the mean is $0$. Now to find the variance,
\begin{align*}
\text{Var}[\Sigma_{i=0}^{n-1} f(t_i)(W_{t_{i+1}} - W_{t_i})] &= \Sigma_{i=0}^{n-1} f(t_i)^2\text{Var}[(W_{t_{i+1}} - W_{t_i})]\\
&= \Sigma_{i=0}^{n-1} f(t_i)^2(t_{i+1} - t_i)
\end{align*}

Now when we take the limit as $n \to \infty$ of the variance, we get,
\begin{align*}
\lim_{n \to \infty} \Sigma_{i=0}^{n-1} f(t_i)^2(t_{i+1} - t_i) = \int_0^T f(t)^2
\end{align*}

Thus, if $Y_t = \int_0^T f(t) dW_t$, then $Y_t \sim N(0, \int_0^T f(t)^2)$.

\begin{problem}{15}
\end{problem}

Let $Z_t = \int_0^t W_s ds$.\\

FINISH THIS LATER.

\begin{problem}{16}
\end{problem}

We have $X_t = \int_0^t (a+\frac{bu}{t}) dW_u$. Since the non-infinitesimal part of the integral does not depend on a random variable, this is a Wiener integral.\\

By Proposition 5.6.1 , we know that Wiener integrals are normal random variables with mean 0 and variance,
\begin{align*}
\int_0^t (a+\frac{bu}{t})^2du &= \int_0^t (a^2 + a\frac{bu}{t} + \frac{b^2u^2}{t^2}) du\\
&= a^2t + a\frac{bt^2}{2t} + \frac{b^2t^3}{3t^2}\\
&= a^2t + a\frac{bt}{2} + \frac{b^2t}{3}\\
&= (a^2 + \frac{ab}{2} + \frac{b^2}{3})t
\end{align*}

So, in order for the variance to equal 1, we require,
\begin{align*}
&(a^2 + \frac{ab}{2} + \frac{b^2}{3})t = 1\\
\implies &a^2 + \frac{ab}{2} + \frac{b^2}{3} = \frac{1}{t}
\end{align*}

\begin{problem}{17}
\end{problem}

FINISH THIS LATER.

\begin{problem}{18}
\end{problem}

FINISH THIS LATER.

\begin{problem}{19}
\end{problem}

Ito's formula states that, if $X_t$ is a stochastic process satisfying $dX_t = b_tdt + \sigma_tdW_t$ with $b_t$ and $\sigma_t$ measurable, and if $F_t = f(X_t)$ with $f$ twice continuously differentiable, we have
\begin{align*}
dF_t = [b_tf'(X_t) + \frac{\sigma^2_t}{2} f''(X_t)]dt + \sigma_tf'(X_t)dW_t
\end{align*}

\begin{enumerate}[\alph*)]
\item In the case that $X_t = W_t$, Corollary 6.2.3, we have the following when we let $F_t = f(W_t)$,
\begin{align*}
dF_t = \frac{1}{2} f''(W_t)dt + f'(W_t)dW_t
\end{align*}

We will apply this formula to the increment below.\\

We have $d(W_te^{W_t})$, so let $f(x) = xe^x$. Then $f'(x) = xe^x + e^x$ and $f''(x) = xe^x + e^x + e^x$. Let $F_t = f(W_t)$ and we have,
\begin{align*}
dF_t &= \frac{1}{2} (W_te^{W_t})'' dt + (W_te^{W_t})' dW_t\\
&= \frac{1}{2} [W_te^{W_t} + e^{W_t} + e^{W_t}] dt + [W_te^{W_t} + e^{W_t}] dW_t\\
&= (\frac{1}{2} W_te^{W_t} + e^{W_t}) dt + e^{W_t}(W_t + 1) dW_t\\
&= e^{W_t} (\frac{1}{2}W_t + 1) dt + e^{W_t}(W_t + 1) dW_t
\end{align*}

\item We have $d(e^{t+W_t^2})$, so let $X_t = t + W_t^2$. Then $dX_t = dt + (2W_tdW_t+dt) = 2dt + 2W_t dW_t$\\

Thus, we have that $b_t = 2$ and $\sigma_t = 2W_t$ satisfying the processes from Ito's lemma. Letting $f(x) = e^x$, we have $f'(x) = f''(x) = e^x$. Now let $F_t = f(X_t)$ and applying Ito's formula yields,
\begin{align*}
dF_t &= [2f'(X_t) + \frac{4W_t^2}{2}f''(X_t)]dt + 2W_t f'(X_t)dW_t\\
&= [2e^{t + W_t^2} + (2W_t^2)e^{t + W_t^2}]dt + 2W_t e^{t + W_t^2} dW_t\\
&= 2e^{t+W_t^2}[1 + W_t^2] dt + 2W_te^{t+W_t^2}dW_tn
\end{align*}

\item We have $d\left(\frac{1}{t^{\alpha}} \int_0^t e^{W_s} d_s\right)$. Then,
\begin{align*}
d\left(\frac{1}{t^{\alpha}} \int_0^t e^{W_s} d_s\right) &= \left(\frac{1}{t^\alpha}\right)' \int_0^t e^{W_s} d_s + \frac{1}{t^\alpha} \left(\int_0^t e^{W_s} d_s\right)'\\
&= \frac{-\alpha}{t^{\alpha+1}} dt \int_0^t e^{W_s} d_s + \frac{1}{t^\alpha} e^{W_t} dt\\
&= \frac{1}{t^{\alpha}} \left[e^{W_t} - \frac{\alpha}{t} \int_0^t e^{W_s} d_s\right]dt
\end{align*}

\end{enumerate}

\begin{problem}{20}
\end{problem}

\begin{enumerate}[\alph*)]
\item We are given $d(t\cos(W_t))$. Then we have that $f(t,x) = t\cos(x)$ and $X_t = W_t$. In addition, we have that $\partial_t f = \cos(x)$, $\partial_x f = -t\sin(x)$, and $\partial^2_x f = -t\cos(x)$.\\

Applying these identities to equation 6.2.7 yields,
\begin{align*}
df(t, X_t) = d(t\cos(W_t)) &= \cos(W_t)dt - t\sin(W_t)dW_t - \frac{1}{2}t\cos(W_t)(dW_t)^2\\
&= \cos(W_t)dt - t\sin(W_t)dW_t - \frac{1}{2}t\cos(W_t)dt\\
&= \cos(W_t)\left[1 - \frac{1}{2}t\right]dt - t\sin(W_t)dW_t
\end{align*}

\item We are given $d(e^tW_t^2)$. Then we have that $f(t,x) = e^tx^2$ and $X_t = W_t$. In addition, we have that $\partial_t f = e^tx^2$, $\partial_x f = 2e^tx$, and $\partial^2_x f = 2e^t$.\\

Applying these identities to equation 6.2.7 yields,
\begin{align*}
df(t, X_t) = d(e^tW_t^2) &= e^tW_t^2dt + 2e^tW_tdW_t + \frac{1}{2}(2e^t)(dW_t)^2\\
&= e^tW_t^2dt + 2e^tW_tdW_t + e^t dt\\
&= e^t\left[W_t^2 + 1\right]dt + 2e^tW_tdW_t
\end{align*}

\item We are given $d(\sin(t)W_t^2)$. Then we have that $f(t,x) = \sin(t)x^2$ and $X_t = W_t$. In addition, we have that $\partial_t f = \cos(t)x^2$, $\partial_x f = 2\sin(t)x$, and $\partial^2_x f = 2\sin(t)$.\\

Applying these identities to equation 6.2.7 yields,
\begin{align*}
df(t, X_t) = d(\sin(t)W_t^2) &= \cos(t)W_t^2dt + 2\sin(t)W_tdW_t + \frac{1}{2}(2\sin(t))(dW_t)^2\\
&= \cos(t)W_t^2dt + 2\sin(t)W_tdW_t + \sin(t)dt\\
&= \left[\cos(t)W_t^2 + \sin(t)\right]dt + 2\sin(t)W_tdW_t
\end{align*}
\end{enumerate}

\begin{problem}{21}
\end{problem}

From Problem 8, we know that for any two independent random variables $X$ and $Y$, we have $E(XY) = E(X) E(Y)$ and $\text{Var}(XY) = \text{Var}(X)\text{Var}(Y) + \text{Var}(X)\left(E(Y)\right)^2 + \text{Var}(Y)\left(E(X)\right)^2$.\\

Now consider two independent Brownian motions $W_t^{(1)}$ and $W_t^{(2)}$. Furthermore, consider the differences for each Brownian motion: $W_{t_1+h}^{(1)} - W_{t_1}^{(1)}$ and $W_{t_2+g}^{(2} - W_{t_2}^{(2)}$ with $h > 0$ and $g > 0$.\\

We know that these differences both have mean 0 and variances $|(t+h) - t| = |h| = h$ and $|(t+g) - t| = |g| = g$, respectively.\\

Now we will take the limit of each difference as $h \to 0^+$ and $g \to 0^+$. In this case, we get that,
\begin{align*}
W_{t_1+h}^{(1)} - W_{t_1}^{(1)} \to dW_t^{(1)}
\end{align*}

as $h \to 0$ and,
\begin{align*}
\text{Var}(W_{t_1+h}^{(1)} - W_{t-1}^{(1)}) = h \to dt_1
\end{align*}

as $h \to 0$.\\

The same holds for the differences of $W_t^{(2)}$.\\

Thus, $dW_t^{(1)} \sim N(0, dt_1)$ and $dW_t^{(2}) \sim N(0, dt_2)$. From Problem 8, we have that,
\begin{align*}
E(dW_t^{(1)} \cdot dW_t^{(2)}) = E(dW_t^{(1)}) \cdot E(dW_t^{(2)}) = 0
\end{align*}

and,
\begin{align*}
\text{Var}(dW_t^{(1)} \cdot dW_t^{(2)}) &= \text{Var}(dW_t^{(1)})\text{Var}(dW_t^{(2)}) + \text{Var}(dW_t^{(1)})\left(E(dW_t^{(2)})\right)^2 + \text{Var}(dW_t^{(2)})\left(E(dW_t^{(1)})\right)^2\\
&= dt_1dt_2 + dt_1(0)^2 + dt_2(0)^2\\
&= dt_1dt_2
\end{align*}

Since $dt_1dt_2$ can be made arbitrarily close to $0$, we can say that $\text{Var}(dW_t^{(1)} \cdot dW_t^{(2)})$.\\

Since the mean of $dW_t^{(1)} \cdot dW_t^{(2)}$ is a constant (0 in this case) and the variance approaches 0, we can say that the mean-squared limit of this quantity is 0. Thus, we have that,
\begin{align*}
dW_t^{(1)} \cdot dW_t^{(2)} = 0
\end{align*}

\begin{problem}{22}
\end{problem}

FINISH THIS LATER.

\begin{problem}{23}
\end{problem}

\begin{enumerate}[\alph*)]

\item We will begin by showing that the derivative of the answer is equal to the integrand.\\

Let $f(t,x) = 1 - e^{t/2}\cos(x)$. Then $\partial_t f = -\frac{1}{2}e^{t/2}\cos(x)$, $\partial_x f = e^{t/2}\sin(x)$, and $\partial^2_x f = e^{t/2}\cos(x)$.\\

Applying these identities to equation 6.2.7 yields,
\begin{align*}
df(t, X_t) = d(1 - e^{t/2}\cos(W_t)) &= -\frac{1}{2}e^{t/2}\cos(W_t)dt + e^{t/2}\sin(W_t)dW_t + \frac{1}{2}e^{t/2}\cos(W_t)(dW_t)^2\\
&= -\frac{1}{2}e^{t/2}\cos(W_t)dt + e^{t/2}\sin(W_t)dW_t + \frac{1}{2}e^{t/2}\cos(W_t)dt\\
&= \frac{1}{2}\left[e^{t/2}\cos(W_t) - e^{t/2}\cos(W_t)\right]dt + e^{t/2}\sin(W_t)dW_t\\
&= \frac{1}{2}[0]dt + e^{t/2}\sin(W_t)dW_t\\
&= e^{t/2}\sin(W_t)dW_t
\end{align*}

So from the problem statement and the above derivation we have that,
\begin{align*}
\int_0^t e^{s/2}\sin(W_s)dW_s &= \int_0^t df(s, W_s)\\
&= f(t, W_t)\\
&= 1 - e^{t/2}\cos(W_t)
\end{align*}

\item Let us take the derivative of the integrated function. By the sum rule of the derivative, we have,
\begin{align*}
d f(W_t) = d\left(\sin(W_t) + \frac{1}{2} \int_0^T \sin(W_t) dt\right) &= d\left[\sin(W_t)\right] + \frac{1}{2} \left[ d\left(\int_0^T \sin(W_t)dt\right) \right]\\
\end{align*}

A direct result of Corollary 6.2.3 is that $d(\sin(W_t)) = \cos(W_t)dW_t - \frac{1}{2}\sin(W_t)dt$. In addition we know that the derivative of the integral over the whole domain (in this case 0 to T) is precisely the integrand. Thus,
\begin{align*}
d f(W_t) = d\left[\sin(W_t)\right] + \frac{1}{2} \left[ d\left(\int_0^T \sin(W_t)dt\right) \right] &= \cos(W_t)dW_t - \frac{1}{2}\sin(W_t)dt + \frac{1}{2}\sin(W_t)dt\\
&= \cos(W_t)dW_t
\end{align*}

So we have,
\begin{align*}
&\int_{0}^T df(W_t) dW_t = f(W_t)\\
\implies &\int_0^T \cos(W_t)dW_t = \sin(W_t) + \frac{1}{2} \int_0^T \sin(W_t) dt
\end{align*}

\item The below formulas are from pages 163 and 164 in the textbook:

\end{enumerate}

\begin{problem}{24}
\end{problem}

By Proposition 8.2.1, we know that if both the drift and volatility of a stochastic differential equation are just functions of time $t$, then the solution is Gaussian distributed with the mean $X_0 + \int_0^t a(s) ds$ and variance $\int_0^t b^2(s) ds$.\\

In the given stochastic differential equation, we see that
\begin{align*}
a(t) = \frac{t}{1+t^2}
\end{align*}

and,

\begin{align*}
b(t) = t^{3/2}
\end{align*}

Thus, we can apply Proposition 8.2.1 in this case. As a result, the mean is given by,
\begin{align*}
X_0 + \int_0^t a(s) ds &= 1 + \int_0^t \frac{s}{1 + s^2} ds\\
&= 1 + \frac{1}{2}\ln(t^2+1)
\end{align*}

and the variance is given by,
\begin{align*}
\int_0^t b^2(s) ds &= \int_0^t \left(s^{3/2}\right)^2 ds\\
&= \int_0^t s^3 ds\\
&= \frac{t^4}{4}
\end{align*}

Thus, the distribution of the solution is given by $X_t \sim N(1 + \frac{1}{2}\ln(t^2+1), \frac{t^4}{4})$.\\

Moreover, the formula for the solution $X_t$, given by 8.1.2, is
\begin{align*}
X_t &= X_0 + \int_0^t a(s) ds + \int_0^t b(s) dW_s\\
&= 1 + \int_0^t \frac{s}{1 + s^2} ds + \int_0^t t^{3/2} dW_s\\
&= 1 + \frac{1}{2}\ln(t^2+1) + \int_0^t t^{3/2} dW_s
\end{align*}

\begin{problem}{25}
\end{problem}

\begin{enumerate}[\alph*)]

\item We are given $dX_t = \cos t dt - \sin t dW_t$, $X_0 = 1$\\

As a result, we have $a(t) = \cos t$ and $b(t) = - \sin t$ and can use Proposition 8.2.1 as above.\\

Hence, the mean is given by,
\begin{align*}
X_0 + \int_0^t a(s) ds &= 1 + \int_0^t \cos(s) ds\\
&= 1 + [\sin(t) - \sin(0)] = 1 + \sin(t)
\end{align*}

and the variance is given by,
\begin{align*}
\int_0^t b^2(s) ds &= \int_0^t \left(-\sin s\right)^2 ds\\
&= \int_0^t \sin^2(s) ds\\
&= \frac{t}{2} - \frac{1}{4}\sin(2t)\\
&= \frac{1}{2}\left[t - \sin(t)\cos(t)\right]
\end{align*}

Thus, $X_t \sim N\left(1 + \sin(t), \frac{1}{2}\left[t - \sin(t)\cos(t)\right]\right)$ and the solution is given by,
\begin{align*}
X_t &= X_0 + \int_0^t a(s) ds + \int_0^t b(s) \, dW_s\\
&= 1 + \int_0^t \cos(s) ds + \int_0^t -\sin s \, dW_s\\
&= 1 + \sin(t) - \int_0^t \sin s \, dW_s
\end{align*}

\item We are given $dX_t = e^t dt + \sqrt{t} dW_t$, $X_0 = 0$\\

As a result, we have $a(t) = e^{t}$ and $b(t) = \sqrt{t}$ and can use Proposition 8.2.1 as above.\\

Hence, the mean is given by,
\begin{align*}
X_0 + \int_0^t a(s) ds &= 0 + \int_0^t e^s ds\\
&= e^t - e^0\\
&= e^t - 1
\end{align*}

and the variance is given by,
\begin{align*}
\int_0^t b^2(s) ds &= \int_0^t \left(\sqrt{s}\right)^2 ds\\
&= \int_0^t s ds\\
&= \frac{t^2}{2}
\end{align*}

Thus, $X_t \sim N\left(e^t - 1, \frac{t^2}{2}\right)$ and the solution is given by,
\begin{align*}
X_t &= X_0 + \int_0^t a(s) ds + \int_0^t b(s) \, dW_s\\
&= 0 + \int_0^t e^s ds + \int_0^t \sqrt{s} \, dW_s\\
&= e^t - 1 + \int_0^t \sqrt{s} \, dW_s
\end{align*}
\end{enumerate}

\begin{problem}{26}
\end{problem}

We have,
\begin{align*}
a(t,x) = 2tx^3 + 3t^2(1+x)
\end{align*}

and
\begin{align*}
b(t,x) = 3t^2x^2 + 1
\end{align*}

So the associated system is,
\begin{align*}
2tx^3 + 3t^2(1+x) &= \partial_t f(t,x) + \frac{1}{2} \partial_x^2 f(t,x)\\
3t^2x^2 + 1 &= \partial_x f(t,x)
\end{align*}

Integrating partially in x in the second equation yields,
\begin{align*}
f(t,x) = \int (3t^2x^2 + 1) dx = t^2x^3 + x + T(t)
\end{align*}

Thus, $\partial_t f = 2tx^3 + T'(t)$. Using the first equation, we have
\begin{align*}
2tx^3 + 3t^2(1+x) = 2tx^3 + T'(t) + 3t^2x
\end{align*}

This implies that $T'(t) = 3t^2$. As a result, $T(t) = t^3 + c$. Hence,
\begin{align*}
f(t,x) = t^2x^3 + x + t^3 + c
\end{align*}

And we have,
\begin{align*}
X_t = f(t, W_t) = t^2(W_t)^3 + W_t + t^3 + c
\end{align*}

Since $X_0 = 0$ is given, we have,
\begin{align*}
X_0 = f(0, W_0) = 0^2(W_0)^3 + W_0 + 0^3 + c = 0
\end{align*}

Thus, $c = 0$. The solution is then,
\begin{align*}
X_t = t^2(W_t)^3 + W_t + t^3
\end{align*}

\begin{problem}{27}
\end{problem}

\begin{enumerate}[\alph*)]

\item We have $a(t,x) = e^t$ and $b(t,x) = x^2 - t$.\\

So the associated system is,
\begin{align*}
e^t &= \partial_t f(t,x) + \frac{1}{2} \partial_x^2 f(t,x)\\
x^2 - t &= \partial_x f(t,x)
\end{align*}

Integrating partially in x in the second equation yields,
\begin{align*}
f(t,x) = \int (x^2 - t) dx = \frac{x^3}{3} - tx + T(t)
\end{align*}

Thus, $\partial_t f = -x + T'(t)$. Using the first equation, we have
\begin{align*}
e^t = -x + T'(t) + x = T'(t)
\end{align*}

This implies that $T'(t) = e^t$. As a result, $T(t) = e^t + c$. Hence,
\begin{align*}
f(t,x) = \frac{x^3}{3} - tx + e^t + c
\end{align*}

Since $X_0 = 1$ is given, we have,
\begin{align*}
X_0 = f(0, W_0) = \frac{W_0^3}{3} - 0W_0 + e^0 + c = 1 + c = 1
\end{align*}

Thus, $c = 0$. The solution is then,
\begin{align*}
X_t = \frac{W_t^3}{3} - t(W_t) + e^t
\end{align*}

\item We have $a(t,x) = \sin t$ and $b(t,x) = x^2 - t$.\\

So the associated system is,
\begin{align*}
\sin t &= \partial_t f(t,x) + \frac{1}{2} \partial_x^2 f(t,x)\\
x^2 - t &= \partial_x f(t,x)
\end{align*}

Integrating partially in x in the second equation yields,
\begin{align*}
f(t,x) = \int (x^2 - t) dx = \frac{x^3}{3} - tx + T(t)
\end{align*}

Thus, $\partial_t f = -x + T'(t)$. Using the first equation, we have
\begin{align*}
\sin t = -x + T'(t) + x = T'(t)
\end{align*}

This implies that $T'(t) = \sin t$. As a result, $T(t) = -\cos t + c$. Hence,
\begin{align*}
f(t,x) = \frac{x^3}{3} - tx - \cos t + c
\end{align*}

Since $X_0 = -1$ is given, we have,
\begin{align*}
X_0 = f(0, W_0) = \frac{W_0^3}{3} - 0W_0 - \cos(0) + c = -1 + c = -1
\end{align*}

Thus, $c = 0$. The solution is then,
\begin{align*}
X_t = \frac{W_t^3}{3} - t(W_t) - \cos t
\end{align*}

\item We have $a(t,x) = t^2$ and $b(t,x) = e^{x-\frac{t}{2}}$\\

So the associated system is,
\begin{align*}
t^2 &= \partial_t f(t,x) + \frac{1}{2} \partial_x^2 f(t,x)\\
e^{x-\frac{t}{2}} &= \partial_x f(t,x)
\end{align*}

Integrating partially in x in the second equation yields,
\begin{align*}
f(t,x) = \int e^{x-\frac{t}{2}} dx = e^{x-\frac{t}{2}} + T(t)
\end{align*}

Thus, $\partial_t f = -\frac{1}{2} e^{x-\frac{t}{2}} + T'(t)$. Using the first equation, we have
\begin{align*}
t^2 = -\frac{1}{2}e^{x-\frac{t}{2}} + T'(t) + \frac{1}{2}e^{x-\frac{t}{2}} = T'(t)
\end{align*}

This implies that $T'(t) = t^2$. As a result, $T(t) = \frac{t^3}{3} + c$. Hence,
\begin{align*}
f(t,x) = e^{x-\frac{t}{2}} + \frac{t^3}{3} + c
\end{align*}

Since $X_0 = 0$ is given, we have,
\begin{align*}
X_0 = f(0, W_0) &= e^{W_0 - \frac{0}{2}} + \frac{0^3}{3} + c\\
&= e^0 + 0 + c\\
&= 1 + c = 0
\end{align*}

Thus, $c = -1$. The solution is then,
\begin{align*}
X_t = e^{W_t-\frac{t}{2}} + \frac{t^3}{3} - 1
\end{align*}

\item We have $a(t,x) = t$ and $b(t,x) = e^{t/2}(\cos x)$\\

So the associated system is,
\begin{align*}
t &= \partial_t f(t,x) + \frac{1}{2} \partial_x^2 f(t,x)\\
e^{t/2}(\cos x) &= \partial_x f(t,x)
\end{align*}

Integrating partially in x in the second equation yields,
\begin{align*}
f(t,x) = \int e^{t/2}(\cos x) dx = e^{t/2}(\sin x) + T(t)
\end{align*}

Thus, $\partial_t f = \frac{1}{2} e^{t/2}(\sin x) + T'(t)$. Using the first equation, we have
\begin{align*}
t = \frac{1}{2}e^{t/2}(\sin x) + T'(t) - \frac{1}{2}e^{t/2}(\sin x) = T'(t)
\end{align*}

This implies that $T'(t) = t$. As a result, $T(t) = \frac{t^2}{2} + c$. Hence,
\begin{align*}
f(t,x) = e^{t/2}(\sin x) + \frac{t^2}{2} + c
\end{align*}

Since $X_0 = 1$ is given, we have,
\begin{align*}
X_0 = f(0, W_0) &= e^{0/2}(\sin W_0) + \frac{0^2}{2} + c\\
&= e^0(\sin 0) + 0 + c\\
&= 0(0) + 0 + c = c = 1
\end{align*}

Thus, $c = 1$. The solution is then,
\begin{align*}
X_t = e^{t/2}(\sin W_t) + \frac{t^2}{2} + 1
\end{align*}

\end{enumerate}

\begin{problem}{28}
\end{problem}

\begin{enumerate}[\alph*)]

\item We have $a(t,x) = x + \frac{3}{2} x^2$ and $b(t,x) = t + x^3$\\

We will first verify the closeness condition:
\begin{align*}
\partial_x a = 1 + 3x
\end{align*}

and,
\begin{align*}
\partial_t b + \frac{1}{2} \partial_x^2 b &= 1 + \frac{1}{2}(6x)\\
&= 1 + 3x
\end{align*}

Thus, we have $\partial_x a = \partial_t b + \frac{1}{2} \partial_x^2 b$ and the closeness condition is satisfied.\\

So the associated system is,
\begin{align*}
x + \frac{3}{2} x^2 &= \partial_t f(t,x) + \frac{1}{2} \partial_x^2 f(t,x)\\
t + x^3 &= \partial_x f(t,x)
\end{align*}

Integrating partially in x in the second equation yields,
\begin{align*}
f(t,x) = \int t + x^3 dx = tx + \frac{x^4}{4} + T(t)
\end{align*}

Thus, $\partial_t f = x + T'(t)$. Using the first equation, we have
\begin{align*}
&x + \frac{3}{2} x^2 = x + T'(t) + \frac{1}{2}3x^2\\
\implies &T'(t) = 0
\end{align*}

This implies that $T'(t) = 0$. As a result, $T(t) = c$. Hence,
\begin{align*}
f(t,x) = tx + \frac{x^4}{4} + c
\end{align*}

Since $X_0 = 0$ is given, we have,
\begin{align*}
X_0 = f(0, W_0) &= 0(W_0) + \frac{W_0^4}{4} + c\\
&= 0 + 0 + c\\
&= c = 0
\end{align*}

Thus, $c = 0$. The solution is then,
\begin{align*}
X_t = t(W_t) + \frac{W_t^4}{4}
\end{align*}

\item We have $a(t,x) = 2tx$ and $b(t,x) = t^2 + x$\\

We will first verify the closeness condition:
\begin{align*}
\partial_x a = 2t
\end{align*}

and,
\begin{align*}
\partial_t b + \frac{1}{2} \partial_x^2 b &= 2t + \frac{1}{2}0\\
&= 2t
\end{align*}

Thus, we have $\partial_x a = \partial_t b + \frac{1}{2} \partial_x^2 b$ and the closeness condition is satisfied.\\

So the associated system is,
\begin{align*}
2tx &= \partial_t f(t,x) + \frac{1}{2} \partial_x^2 f(t,x)\\
t^2 + x &= \partial_x f(t,x)
\end{align*}

Integrating partially in x in the second equation yields,
\begin{align*}
f(t,x) = \int (t^2 + x) dx = t^2x + \frac{x^2}{2} + T(t)
\end{align*}

Thus, $\partial_t f = 2tx + T'(t)$. Using the first equation, we have
\begin{align*}
&2tx = 2tx + T'(t) + \frac{1}{2}1\\
\implies &T'(t) = -\frac{1}{2}
\end{align*}

This implies that $T'(t) = -\frac{1}{2}$. As a result, $T(t) = -\frac{1}{2}t + c$. Hence,
\begin{align*}
f(t,x) = t^2x + \frac{x^2}{2} - \frac{1}{2}t + c
\end{align*}

Since $X_0 = 0$ is given, we have,
\begin{align*}
X_0 = f(0, W_0) &= 0^2(W_0) + \frac{W_0^2}{2} - \frac{1}{2}0 + c\\
&= 0 + 0 + 0 + c\\
&= c = 0
\end{align*}

Thus, $c = 0$. The solution is then,
\begin{align*}
X_t = t^2(W_t) + \frac{W_t^2}{2} - \frac{1}{2}t
\end{align*}

\item We have $a(t,x) = e^tx + \frac{1}{2}\cos x$ and $b(t,x) = e^t + \sin x$\\

We will first verify the closeness condition:
\begin{align*}
\partial_x a = e^t - \frac{1}{2}\sin x
\end{align*}

and,
\begin{align*}
\partial_t b + \frac{1}{2} \partial_x^2 b &= e^t - \frac{1}{2}\sin x
\end{align*}

Thus, we have $\partial_x a = \partial_t b + \frac{1}{2} \partial_x^2 b$ and the closeness condition is satisfied.\\

So the associated system is,
\begin{align*}
e^tx + \frac{1}{2}\cos x &= \partial_t f(t,x) + \frac{1}{2} \partial_x^2 f(t,x)\\
e^t + \sin x &= \partial_x f(t,x)
\end{align*}

Integrating partially in x in the second equation yields,
\begin{align*}
f(t,x) = \int (e^t + \sin x) dx = e^tx - \cos x + T(t)
\end{align*}

Thus, $\partial_t f = e^tx + T'(t)$. Using the first equation, we have
\begin{align*}
&e^tx + \frac{1}{2}\cos x = e^tx + T'(t) + \frac{1}{2}\cos x\\
\implies &T'(t) = -\frac{1}{2}
\end{align*}

This implies that $T'(t) = 0$. As a result, $T(t) = c$. Hence,
\begin{align*}
f(t,x) = e^tx - \cos x + c
\end{align*}

Since $X_0 = 0$ is given, we have,
\begin{align*}
X_0 = f(0, W_0) &= e^0(W_0) - \cos(W_0) + c\\
&= 1(0) - 1 + c\\
&= -1 + c = 0
\end{align*}

Thus, $c = 1$. The solution is then,
\begin{align*}
X_t = e^t(W_t) - \cos(W_t) + 1
\end{align*} 

\item We have $a(t,x) = e^x(1 + \frac{t}{2})$ and $b(t,x) = te^x$\\

We will first verify the closeness condition:
\begin{align*}
\partial_x a = e^x(1 + \frac{t}{2})
\end{align*}

and,
\begin{align*}
\partial_t b + \frac{1}{2} \partial_x^2 b &= e^x + \frac{1}{2}te^x\\
&= e^x(1 + \frac{t}{2})
\end{align*}

Thus, we have $\partial_x a = \partial_t b + \frac{1}{2} \partial_x^2 b$ and the closeness condition is satisfied.\\

So the associated system is,
\begin{align*}
e^x(1 + \frac{t}{2}) &= \partial_t f(t,x) + \frac{1}{2} \partial_x^2 f(t,x)\\
te^x &= \partial_x f(t,x)
\end{align*}

Integrating partially in x in the second equation yields,
\begin{align*}
f(t,x) = \int (te^x) dx = te^x + T(t)
\end{align*}

Thus, $\partial_t f = e^x + T'(t)$. Using the first equation, we have
\begin{align*}
&e^x(1 + \frac{t}{2}) = e^x + T'(t) + \frac{1}{2}te^x\\
\implies &e^x(1 + \frac{t}{2}) = e^x(1 + \frac{t}{2}) + T'(t)\\
\implies &T'(t) = 0
\end{align*}

This implies that $T'(t) = 0$. As a result, $T(t) = c$. Hence,
\begin{align*}
f(t,x) = te^x + c
\end{align*}

Since $X_0 = 2$ is given, we have,
\begin{align*}
X_0 = f(0, W_0) &= 0e^{W_0} + c\\
&= 0(1) + c\\
&= c = 2
\end{align*}

Thus, $c = 2$. The solution is then,
\begin{align*}
X_t = te^{W_t} + 2
\end{align*}

\end{enumerate}

\begin{problem}{29}
\end{problem}

\begin{problem}{30}
\end{problem}

\begin{problem}{31}
\end{problem}

\begin{enumerate}[\alph*)]

\item We have $dX_t = \alpha X_t dW_t$ with $\alpha$ a constant. The integrating factor is given by,
\begin{align*}
\rho_t &= e^{-\int_0^t g(s) dW_s + \frac{1}{2}\int_0^t g^2(s) ds}\\
&= e^{-\int_0^t \alpha dW_s + \frac{1}{2}\int_0^t \alpha^2 ds}\\
&= e^{\frac{1}{2} \alpha^2t - \alpha W_t}
\end{align*}

By Ito's formula, we have that,
\begin{align*}
d\rho_t = \rho_t(\alpha^2 dt - \alpha dW_t)
\end{align*}

Using $dt^2 = dt dW_t = 0$ and $(dW_t)^2 = dt$, we obtain,
\begin{align*}
dX_td\rho_t &= \alpha X_t dW_t[\rho_t(\alpha^2 dt - \alpha dW_t)]\\
&= \alpha X_t dW_t[\rho_t \alpha^2 dt - \rho_t \alpha dW_t]\\
&= \alpha X_t [\rho_t \alpha^2 dW_t dt - \rho_t \alpha (dW_t)^2]\\
&= \alpha X_t [-\rho_t \alpha dt]\\
&= -\alpha^2 \rho_t X_t dt
\end{align*}

Multiplying by $\rho_t$, the initial equation becomes,
\begin{align*}
\rho_tdX_t - \alpha\rho_tX_tdW_t = 0
\end{align*}

and adding and subtracting $\alpha^2\rho_tX_tdt$ from the left side yields,
\begin{align*}
\rho_tdX_t - \alpha\rho_tX_tdW_t + \alpha^2\rho_tX_tdt - \alpha^2\rho_tX_tdt = 0
\end{align*}

This can be written as,
\begin{align*}
\rho_tdX_t + X_td\rho_t + d\rho_t dX_t = 0
\end{align*}

which, by virtue of the product rule, becomes
\begin{align*}
d(\rho_tX_t) = 0
\end{align*}

Integrating yields,
\begin{align*}
\rho_tX_t = \rho_0X_0 + \int_0^t 0 ds &= \rho_0X_0\\
&= (e^{\frac{1}{2} \alpha^2(0) - \alpha W_0})X_0\\
&= (e^0)X_0 = X_0
\end{align*}

And hence the solution is,
\begin{align*}
X_t &= \frac{X_0}{\rho_t}\\
&= \frac{X_0}{e^{\frac{1}{2} \alpha^2t - \alpha W_t}}
\end{align*}

\item We have $dX_t = X_t dt + \alpha X_t dW_t$ with $\alpha$ a constant. The integrating factor is given by,
\begin{align*}
\rho_t &= e^{-\int_0^t g(s) dW_s + \frac{1}{2}\int_0^t g^2(s) ds}\\
&= e^{-\int_0^t \alpha dW_s + \frac{1}{2}\int_0^t \alpha^2 ds}\\
&= e^{\frac{1}{2} \alpha^2t - \alpha W_t}
\end{align*}

By Ito's formula, we have that,
\begin{align*}
d\rho_t = \rho_t(\alpha^2 dt - \alpha dW_t)
\end{align*}

Using $dt^2 = dt dW_t = 0$ and $(dW_t)^2 = dt$, we obtain,
\begin{align*}
dX_td\rho_t &= [X_tdt + \alpha X_t dW_t][\rho_t(\alpha^2 dt - \alpha dW_t)]\\
&= [X_tdt + \alpha X_t dW_t][\rho_t \alpha^2 dt - \rho_t \alpha dW_t]\\
&=  \rho_t X_t \alpha^2 (dt)^2 - \rho_t X_t \alpha dt dW_t + \rho_t X_t \alpha^3 dt dW_t - \rho_t X_t \alpha^2 (dW_t)^2\\
&= - \alpha^2 \rho_t X_t dt
\end{align*}

Multiplying by $\rho_t$, the initial equation becomes,
\begin{align*}
\rho_tdX_t - \alpha\rho_tX_tdW_t = X_t dt
\end{align*}

and adding and subtracting $\alpha^2\rho_tX_tdt$ from the left side yields,
\begin{align*}
\rho_tdX_t - \alpha\rho_tX_tdW_t + \alpha^2\rho_tX_tdt - \alpha^2\rho_tX_tdt = \rho_tX_t dt
\end{align*}

which, by virtue of the product rule, becomes
\begin{align*}
d(\rho_tX_t) = \rho_t X_t dt
\end{align*}

So, with $Y_t = \rho_tX_t$, we have
\begin{align*}
d(Y_t) = Y_t dt
\end{align*}

\item

\end{enumerate}

\end{document}