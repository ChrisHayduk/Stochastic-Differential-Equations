\documentclass[12pt]{article}
 
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amsthm,amssymb, mathtools}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{fixltx2e}
\usepackage[shortlabels]{enumitem}
\usepackage{mathrsfs}
 
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}

\newcommand{\stcomp}[1]{{#1}^\complement}
 
\newenvironment{theorem}[2][Theorem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{lemma}[2][Lemma]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{exercise}[2][Exercise]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{question}[2][Question]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{corollary}[2][Corollary]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newcommand{\textfrac}[2]{\dfrac{\text{#1}}{\text{#2}}}

\begin{document}

\title{Stochastic Differential Equations: Final Project}

\author{Chris Hayduk}
\date{\today}

\maketitle

\textbf{NOTE: Still need to complete Problems 2, 7, 8}

\begin{problem}{1}
\end{problem}

We know that the probability density of the sum of two independent random variables can be computed by the convolutions of $f_X$ and $f_Y$. Thus, the probability density of $X + Y$, denoted by $f_Z$, is
\begin{align}
f_Z(z) = (f_Y*f_X) = \int_{-\infty}^{\infty}  f_Y(z - x)f_X(x) dx
\end{align}

We know that the normal density function is given by,
\begin{align*}
f(x) = \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2}
\end{align*}

Plugging this density function into equation (1) and letting $\sigma_Z = \sqrt{\sigma_X^2 + \sigma_Y^2}$ yields,
\begin{align*}
f_Z(z) &= \int_{-\infty}^{\infty} \left[\frac{1}{\sigma_Y\sqrt{2\pi}} e^{-\frac{1}{2}\left(\frac{(z-x)-\mu_Y}{\sigma_Y}\right)^2} \frac{1}{\sigma_X\sqrt{2\pi}} e^{-\frac{1}{2}\left(\frac{x-\mu_X}{\sigma_X}\right)^2}\right] dx\\
&= \int_{-\infty}^{\infty} \left[\frac{1}{\sigma_Y\sigma_X(2\pi)} e^{-\frac{1}{2}\left[\left(\frac{z-x-\mu_Y}{\sigma_Y}\right)^2 + \left(\frac{x-\mu_X}{\sigma_X}\right)^2 \right]} \right] dx\\
&= \int_{-\infty}^{\infty} \left[\frac{1}{\sigma_Y\sigma_X(2\pi)} e^{-\frac{\sigma_X^2(z-x-\mu_Y)^2 + \sigma_Y^2(x-\mu_X)^2}{2 \sigma_Y^2 \sigma_X^2}} \right] dx\\
&= \int_{-\infty}^\infty \left[\frac{1}{(2\pi)\sigma_X\sigma_Y} e^{
      -\frac
         {
            x^2(\sigma_X^2 + \sigma_Y^2) - 
            2x(\sigma_X^2(z - \mu_Y) + \sigma_Y^2\mu_X) +
            \sigma_X^2(z^2 + \mu_Y^2 - 2z\mu_Y) + \sigma_Y^2\mu_X^2
         }
         {2\sigma_Y^2\sigma_X^2}}\right] dx\\
&= \int_{-\infty}^\infty
   \left[ \frac{1}{\sqrt{2\pi}\sigma_Z}
    e^{-\frac
         {
            \sigma_Z^2\left(\sigma_X^2(z - \mu_Y)^2 + \sigma_Y^2\mu_X^2\right) -
            \left(\sigma_X^2(z - \mu_Y) + \sigma_Y^2\mu_X\right)^2
         }
         {2\sigma_Z^2\left(\sigma_X\sigma_Y\right)^2}}
   \frac{1}{\sqrt{2\pi}\frac{\sigma_X\sigma_Y}{\sigma_Z}}
    e^{-\frac
         {
            \left(x - \frac{\sigma_X^2(z - \mu_Y) + \sigma_Y^2\mu_X}{\sigma_Z^2}\right)^2
         }
         {2\left(\frac{\sigma_X\sigma_Y}{\sigma_Z}\right)^2}} \right]
    dx \\
&= \frac{1}{\sqrt{2\pi}\sigma_Z}
   e^{- { (z-(\mu_X+\mu_Y))^2 \over 2\sigma_Z^2 }}
   \int_{-\infty}^{\infty} \left[
   \frac{1}{\sqrt{2\pi}\frac{\sigma_X\sigma_Y}{\sigma_Z}}
   e^{- \frac{\left(x-\frac{\sigma_X^2(z-\mu_Y)+\sigma_Y^2\mu_X}{\sigma_Z^2}\right)^2}{2\left(\frac{\sigma_X\sigma_Y}{\sigma_Z}\right)^2}} \right]
    dx
\end{align*}

The equation inside the integral symbol represents a valid normal density function for $x$, so we know it integrates to $1$. Thus, the probability density function for $X + Y$ is given by
\begin{align*}
f_Z(z) = \frac{1}{\sqrt{2\pi}\sigma_Z}
   e^{- { (z-(\mu_X+\mu_Y))^2 \over 2\sigma_Z^2 }}
\end{align*}

This is precisely a normal density function with mean $\mu_X + \mu_Y$ and variance $\sigma_Z^2 = (\sqrt{\sigma_X^2 + \sigma_Y^2})^2 = \sigma_X^2 + \sigma_Y^2$.\\

Hence, we have shown that, given $X \sim N(\mu_X, \sigma_X^2)$ and $Y \sim N(\mu_Y, \sigma_Y^2)$, $X + Y$ is distributed as $N(\mu_X + \mu_Y, \sigma_X^2 + \sigma_Y^2)$.

\begin{problem}{2}
\end{problem}

Let $Z = Y + 1$. We'll begin by finding the probability density function for $Z$. Since $Y$ is a standard uniform random variable, we know that
\begin{align*}
f(y) = \begin{cases} 
      1 & 0 \leq y \leq 1 \\
      0 & \text{elsewhere} 
   \end{cases}
\end{align*}

Thus, the distribution function approach yields,
\begin{align*}
F_Z(z) = P(Z \leq z) = P(Y + 1 \leq z) &= P(Y \leq z - 1)\\
&= \int_{-\infty}^{z-1} f(y)dy \\
&= \int_0^{z-1} 1 \, dy\\
&= z-1
\end{align*}

So, since as $Y$ ranges from 0 to 1, $Z$ ranges from 1 to 2, we have
\begin{align*}
F_Z(z) = \begin{cases} 
      0 & z < 1 \\
      z-1 & 1 \leq z \leq 2\\
      1 & \text{elsewhere} 
   \end{cases}
\end{align*}

And the density function for $Z$ is
\begin{align*}
f_Z(z) = \frac{dF_Z(z)}{dz} = \begin{cases} 
      1 & 1 \leq z \leq 2\\
      0 & \text{elsewhere} 
   \end{cases}
\end{align*}

Now we can simplify the original function to be $U = X/Z$, where we now know the probability density function for both random variables ($X \sim \text{Unif}(0,1)$ and $Z \sim \text{Unif}(1, 2)$).\\

So the distribution function is given by,
\begin{align*}
F_U(u) = P(U \leq u) = P(X/Z \leq u) &= P(X \leq uZ)\\
&= \int_{-\infty}^{uz} f(x)dx \\
&= \int_0^{uz} 1 \, dx\\
&= uz
\end{align*}

FINISH THIS LATER.

\begin{problem}{3}
\end{problem}

Let $Y$ be a standard normal variable. Then the moment-generating function of $Y$ is given by,
\begin{align*}
m(t) &= E(e^{tY})\\
&= \int_{-\infty}^{\infty} e^{ty}f(y) \, dy\\
\end{align*}

Since $Y$ is a standard normal variable, we know that $\mu = 0$ and $\sigma = 1$. Thus, we have 
\begin{align*}
f(y) = \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}y^2}
\end{align*}

Plugging this into the above equation yields,
\begin{align*}
m(t) &= \int_{-\infty}^{\infty} e^{ty} \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}y^2} \, dy\\
&= \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} e^{-\frac{y^2}{2} + ty} \, dy\\
&= \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}(y - t)^2} e^{\frac{1}{2} t^2} \, dy\\
&= e^{\frac{1}{2} t^2} \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}(y - t)^2} \, dy
\end{align*}

We can see that the integral is precisely the integral for a normal random variable with $\mu = y - t$ and $\sigma = 1$. Thus, it integrates to $1$, yielding,
\begin{align*}
m(t) = e^{\frac{1}{2} t^2}
\end{align*}

\begin{problem}{4}
\end{problem}

Suppose $g(x)$ is a monotone increasing function and $X$ is a random variable with the probability density function $f_X$.\\

Let $U = g(X)$ where $X$ has the above density function. Since $g(x)$ is an increasing function of $x$, then $g^{-1}(u)$ is an increasing function of $u$. Thus,
\begin{align*}
P(U \leq u) &= P[g(X) \leq u]\\
&= P\{g^{-1}[g(X)] \leq g^{-1}(u)\}\\
&= P[X \leq g^{-1}(u)]
\end{align*}

The above sequence of equalities implies that,
\begin{align*}
F_U(u) = F_X[g^{-1}(u)]
\end{align*}

When we differentiate with respect to $u$, we get,
\begin{align*}
f_U(u) = \frac{dF_X[g^{-1}(u)]}{du} = f_X(g^{-1}(u))\frac{d[g^{-1}(u)]}{du}
\end{align*}

\begin{problem}{5}
\end{problem}

We know that the moment-generating function of $X$ is $\phi(t)$. Thus, 
\begin{align*}
\phi(t) &= E(e^{tX})\\
&= \int_{-\infty}^{\infty} e^{tx} f(x) \, dx\\
&= \int_{-\infty}^{\infty} \left(1 + tx + \frac{t^2x^2}{2!} + \frac{t^3x^3}{3!} + \cdots \right) f(x) \, dx\\
&= \int_{-\infty}^{\infty} f(x) dx + t \int_{-\infty}^{\infty} xf(x)dx + \frac{t^2}{2!} \int_{-\infty}^{\infty} x^2f(x)dx + \cdots
\end{align*}

Now plugging in $aX + b$ to the above equation yields,
\begin{align*}
E(e^{t(aX+b)}) &= E(e^{taX + tb})\\
&= E[e^{taX}(e^{tb})]
\end{align*}

Since expected value is a linear operator and $e^{tb}$ is a constant, we can pull it out of the expectation operator,
\begin{align*}
E[e^{taX}(e^{tb})] &= e^{tb} E(e^{taX})\\
&= e^{tb} \int_{-\infty}^{\infty} e^{tax} f(ax) \, dx
\end{align*}

We can see from the above equation that this is equal to $e^{tb}\phi(ta)$.\\

Thus, the moment-generating function for $aX + b$ with constants $a \neq 0$ and $b$ is $e^{tb}\phi(ta)$.

\begin{problem}{6}
\end{problem}

We know by Theorem 2.11.1 that the distribution of the sum of two random variables $X$ and $Y$ is given by the convolution of their densities, $f_X$ and $f_Y$.\\

Thus, with $Z = X + Y$, we have,
\begin{align*}
f_Z(z) = (f*g)(z) = \int_{-\infty}^{\infty} f_X(z-y)f_Y(y) dy
\end{align*}

The mean of this distribution is,
\begin{align*}
E(Z) = \int_{-\infty}^{\infty} zf_X(z-y)f_Y(y) dy
\end{align*}

where $z = x+y$. Thus, we have,
\begin{align*}
E(Z) &= \int_{-\infty}^{\infty} zf_Z(z) dz\\
&= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \left[(x+y)f_X((x+y)-y)f_Y(y)\right] dxdy\\
&= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \left[(x+y)f_X(x)f_Y(y)\right] dxdy\\
&= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \left[(xf_X(x) + yf_X(x))f_Y(y)\right] dxdy\\
&= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \left[xf_X(x)f_Y(y) + yf_X(x)f_Y(y)\right] dxdy\\
&= \int_{-\infty}^{\infty} \left[\mu_Xf_Y(y) + yf_Y(y)\right]dy\\
&= \mu_X + \mu_Y
\end{align*}

In the above equations, we are using the properties that, for any random variable $X$ with density function $f(x)$ and mean $\mu$, we have that $\int_{-\infty}^{\infty} f(x) = 1$ and  $\int_{-\infty}^{\infty} xf(x) = \mu$.\\

Now for the variance, we know that for a given random variable $X$, $\text{Var}(X) = E[X^2] - E[X]^2$. Plugging in $Z$ to this formula yields,
\begin{align*}
\text{Var}(Z) = \sigma_Z^2 &= E[Z^2] - E[Z]^2\\
&= \int_{-\infty}^{\infty} z^2f_Z(z) dz + (\mu_X + \mu_Y)^2\\
&= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \left[(x+y)^2f_X((x+y)-y)f_Y(y)\right] dxdy - (\mu_X + \mu_Y)^2\\
&= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \left[(x+y)^2f_X(x)f_Y(y)\right] dxdy - (\mu_X + \mu_Y)^2\\
&= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \left[(x^2 + 2xy + y^2)f_X(x)f_Y(y)\right] dxdy - (\mu_X + \mu_Y)^2\\
&= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} \left[x^2f_X(x)f_Y(y) + 2xyf_X(x)f_Y(y) + y^2f_X(x)f_Y(y)\right] dxdy - (\mu_X + \mu_Y)^2\\
&= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} x^2f_X(x)f_Y(y) dxdy - \mu_X + \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} 2xyf_X(x)f_Y(y) dxdy\; + \\&\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} y^2f_X(x)f_Y(y) dxdy - \mu_Y\\
&= \left[\int_{-\infty}^{\infty} x^2f_X(x)dx - \mu_X\right] + 2\mu_X\mu_Y + \left[\int_{-\infty}^{\infty} y^2f_Y(y)dy - \mu_Y\right]\\
&= \sigma_X^2 + 2\mu_X\mu_Y + \sigma_Y^2
\end{align*}

Thus, we have $\text{Var}(Z) = \sigma_X^2 + 2\mu_X\mu_Y + \sigma_Y^2$.

\begin{problem}{7}
\end{problem}

\noindent The definition of the Central Limit Theorem is as follows:\\
\\
Let $Y_1, Y_2, ..., Y_n$ be independent and identically distributed random variables with $E(Y_i) = \mu$ and $V(Y_i) = \sigma^2 < \infty$. Define
\begin{align*}
U_n = \frac{\Sigma_{i=1}^n Y_i - n\mu}{\sigma\sqrt{n}} = \frac{\overline{Y} - \mu}{\sigma/\sqrt{n}}
\end{align*}

\noindent where $\overline{Y} = \frac{1}{n} \Sigma_{i=1}^n Y_i$.\\

\noindent Then the distribution function of $U_n$ converges to the standard normal distribution function as $n \to \infty$. That is,
\begin{align*}
\lim_{n \to \infty} P(U_n \leq u) = \int_{-\infty}^{u} \frac{1}{\sqrt{2\pi}}e^{-t^2/2}dt
\end{align*}

\noindent for all $u$.\\

\noindent Let $f_n(x)$ be the probability mass function for a binomial random variable with $n$ trials and success probability $p$.

FINISH THIS LATER.

\begin{problem}{8}
\end{problem}

Let $X$ and $Y$ be two independent, continuous random variables described by probability density functions $f_X$ and $f_Y$. Also let $Z = XY$. We'll begin by finding the cumulative distribution function for $Z$. This yields,
\begin{align*}
F_Z(z) &= P(Z \leq z)\\
&= P(XY \leq z)\\
&= P(XY \leq z, X \geq 0) + P(XY \leq z, X \leq 0)\\
&= P(Y \leq z/X, X \geq 0) + P(Y \geq z/X, X \leq 0)\\
&= \int_{0}^{\infty} f_X(x) \int_{-\infty}^{z/x} f_Y(y) dy dx + \int_{-\infty}^0 f_X(x) \int_{z/X}^{\infty} f_Y(y) dy dx
\end{align*}

Now in order to find the probability density function for $Z$, we need to differentiate with respect to $z$ on both sides of the above equation.
\begin{align*}
f_Z(z) &= \frac{d}{dz}F_z(z)\\
&= \frac{d}{dz}\left[\int_{0}^{\infty} f_X(x) \int_{-\infty}^{z/x} f_Y(y) dy dx + \int_{-\infty}^0 f_X(x) \int_{z/X}^{\infty} f_Y(y) dy dx\right]\\
&= \int_{0}^{\infty} f_X(x) \left[f_Y(z/x)\left(\frac{1}{x}\right) - f(-\infty)\right] dx + \int_{-\infty}^{0} f_X(x) \left[f_Y(\infty) - f_Y(z/x)\left(\frac{1}{x}\right)\right] dx
\end{align*}

We obtained the above equation using the fundamental theorem of calculus and the chain rule. Now, we will use the fact that if $f(x)$ is a distribution function, as $x \to \infty$, $f(x) \to 0$. The same holds true as $x \to -\infty$. This yields,
\begin{align*}
f_Z(z) &= \int_{0}^{\infty} f_X(x) \left[f_Y(z/x)\left(\frac{1}{x}\right) - f(-\infty)\right] dx + \int_{-\infty}^{0} f_X(x) \left[f_Y(\infty) - f_Y(z/x)\left(\frac{1}{x}\right)\right] dx\\
&= \int_{0}^{\infty} f_X(x)f_Y(z/x)\frac{1}{x} dx + \int_{-\infty}^{0} f_X(x)(-f_Y(z/x))\frac{1}{x} dx\\
&= \int_{0}^{\infty} f_X(x)f_Y(z/x)\frac{1}{x} dx - \int_{-\infty}^{0} f_X(x)f_Y(z/x)\frac{1}{x} dx\\
&= \int_{0}^{\infty} f_X(x)f_Y(z/x)\frac{1}{|x|} dx + \int_{-\infty}^{0} f_X(x)f_Y(z/x)\frac{1}{|x|} dx\\
&= \int_{-\infty}^{\infty} f_X(x)f_Y(z/x)\frac{1}{|x|} dx
\end{align*}

Now we need to find the mean of $Z$ where $Z = XY$,
\begin{align*}
E[Z] &= \int_{-\infty}^{\infty} zf_Z(z)dz\\
&= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} (xy)f_X(x)f_Y(y)\frac{1}{|x|} dxdy\\
&= \mu_Y \int_{-\infty}^{\infty} \frac{x}{|x|}f_X(x)dx\\
&= \mu_Y \left[\int_{-\infty}^0 - f_X(x) dx + \int_{0}^{\infty} f_X(x) dx\right]\\
&= \mu_Y \left[-\int_{-\infty}^0 f_X(x) dx + \int_{0}^{\infty} f_X(x) dx\right]
\end{align*}

FINISH THIS LATER.

\begin{problem}{9}
\end{problem}

We have that,
\begin{align*}
t_j - t_{j-1} &= (j/N)(b-a) + a - [(j-1/N)(b-a) + a]\\
&= (jb - ja)/N + a - (jb - ja - b + a)/N - a\\
&= (jb - ja - jb + ja + b - a)/N\\
&= (b - a)/N
\end{align*}

In addition, since the function $f$ is bounded on $[a,b]$, we know that $\exists M \in \mathbb{N}$ such that $M \geq |f|$. These two facts yield the following,
\begin{align*}
\lim_{N \to \infty} \Sigma^N_{j=1} f(t_{j-1})(t_j - t_{j-1})^p &= \lim_{N \to \infty} \Sigma^N_{j=1} f(t_{j-1})((b - a)/N)^p\\
&\leq \lim_{N \to \infty} \Sigma^N_{j=1} M((b - a)/N)^p\\
&= \lim_{N \to \infty} MN((b - a)/N)^p\\
&= M \lim_{N \to \infty} N((b - a)/N)^p\\
&= M(b-a)^p \lim_{N \to \infty} N/N^p\\
&= M(b-a)^p \lim_{N \to \infty} N^{1-p}
\end{align*}

Since $1-p < 0$, we have that $\lim_{N \to \infty} N^{1-p} = 0$. Thus, this gives us,
\begin{align*}
\lim_{N \to \infty} \Sigma^N_{j=1} f(t_{j-1})(t_j - t_{j-1})^p &\leq M(b-a)^p \lim_{N \to \infty} N^{1-p}\\
&= [M(b-a)]0 = 0
\end{align*}

Now, if we use $-M$ for the lower bound, we get,
\begin{align*}
\lim_{N \to \infty} \Sigma^N_{j=1} f(t_{j-1})(t_j - t_{j-1})^p &\geq -M(b-a)^p \lim_{N \to \infty} N^{1-p}\\
&= [-M(b-a)]0 = 0
\end{align*}

Since we have shown that
\begin{align*}
0 \leq \lim_{N \to \infty} \Sigma^N_{j=1} f(t_{j-1})(t_j - t_{j-1})^p \leq 0
\end{align*}

we have that $\lim_{N \to \infty} \Sigma^N_{j=1} f(t_{j-1})(t_j - t_{j-1})^p = 0$.

\begin{problem}{10}
\end{problem}

We know, by Proposition 4.9.1, that for a given stochastic process $X_t$, if $E[X_t] \to k$ a constant and $\text{Var}(X_t) \to 0$ as $t \to \infty$, then $\operatorname{ms-\lim}_{t \to \infty} X_t = k$.\\

Thus, we will need to check the mean and variance of the given function. We will begin with the mean. Using the fact that the function $f$ is a constant in terms of the expected value function, we get
\begin{align*}
E(\lim_{N \to \infty} \Sigma^N_{j=1} f(t_{j-1}) (\Delta_{j-1}^j W)^2) = \lim_{N \to \infty} \Sigma^N_{j=1} f(t_{j-1}) E[(\Delta_{j-1}^j W)^2]
\end{align*}

We know that the expected value of the square of a random variable is its variance, which is $t_j - t_{j-1}$ in this case. Hence,
\begin{align*}
E(\lim_{N \to \infty} \Sigma^N_{j=1} f(t_{j-1}) (\Delta_{j-1}^j W)^2) &= \lim_{N \to \infty} \Sigma^N_{j=1} f(t_{j-1}) E[(\Delta_{j-1}^j W)^2]\\
&= \lim_{N \to \infty} \Sigma^N_{j=1} f(t_{j-1})(t_j - t_{j-1})
\end{align*}

This is the equation for the Riemann integral of $f$ over $[a,b]$. Since we already know $f$ is bounded and defined on a closed interval, as long as $f$ is continuous almost everywhere on $[a,b]$ we assert that the above limit exists and is equal to the integral of $f$. If that is the case, then we have
\begin{align*}
E(\lim_{N \to \infty} \Sigma^N_{j=1} f(t_{j-1}) (\Delta_{j-1}^j W)^2) &= \lim_{N \to \infty} \Sigma^N_{j=1} f(t_{j-1})(t_j - t_{j-1})\\
&= \int_a^b f(t) dt
\end{align*}

Now we need to show that the variance of the function is 0. Using Problem 8 to show that $\text{Var}[(\Delta_{j-1}^j W)^2] = 2(t_j - t_{j-1})^2$, we have
\begin{align*}
\text{Var}(\lim_{N \to \infty} \Sigma^N_{j=1} f(t_{j-1}) (\Delta_{j-1}^j W)^2) &= \lim_{N \to \infty} \Sigma^N_{j=1} (f(t_{j-1}))^2 \text{Var}[(\Delta_{j-1}^j W)^2]\\
&= \lim_{N \to \infty} \Sigma^N_{j=1} (f(t_{j-1}))^2 \cdot 2(t_j - t_{j-1})^2\\
&= 2\lim_{N \to \infty} \Sigma^N_{j=1} (f(t_{j-1}))^2 (t_j - t_{j-1})^2
\end{align*}

Now observe that if $M$ is an upper bound for $|f|$ on $[a,b]$ (as in Problem 9), then $M^2$ is an upper bound for $|f|^2$ on $[a,b]$. This is true because increasing functions preserve inequalities, squaring is an increasing function for $x \geq 0$, and $|f| \geq 0$ for every $x$. In addition, observe that $|f|^2 = f^2$. Thus, we have that $M^2$ is an upper bound for $f^2$ on $[a,b]$.\\

We can now the above and the result from Problem 9 in order to obtain the following,
\begin{align*}
\text{Var}(\lim_{N \to \infty} \Sigma^N_{j=1} f(t_{j-1}) (\Delta_{j-1}^j W)^2) &= 2\lim_{N \to \infty} \Sigma^N_{j=1} (f(t_{j-1}))^2 (t_j - t_{j-1})^2\\
&\leq 2M^2 \lim_{N \to \infty} \Sigma^N_{j=1} (t_j - t_{j-1})^2\\
&= 2M^2(0) = 0
\end{align*}

In addition, since we know that variance is always $\geq 0$, we have that,
\begin{align*}
\text{Var}(\lim_{N \to \infty} \Sigma^N_{j=1} f(t_{j-1}) (\Delta_{j-1}^j W)^2) = 0
\end{align*}

Thus, we have shown that this function satisfies Proposition 4.9.1 and can state that $\operatorname{ms-\lim}_{t \to \infty} [\lim_{N \to \infty} \Sigma^N_{j=1} f(t_{j-1}) (\Delta_{j-1}^j W)^2] = \int_a^b f(t) dt$.

\begin{problem}{11}
\end{problem}

We need to show that the mean and variance of this function are $0$. We'll begin with the mean,
\begin{align*}
E[\lim_{N \to \infty} \Sigma^N_{j=1} f(t_{j-1})(t_j - t_{j-1})^p(\Delta_{j-1}^j W)^q] &= \lim_{N \to \infty} \Sigma^N_{j=1} f(t_{j-1})(t_j - t_{j-1})^p E[(\Delta_{j-1}^j W)^q]
\end{align*}

If $q$ is odd, then $E[(\Delta_{j-1}^j W)^q] = 0$. If $q$ is even, then $E[(\Delta_{j-1}^j W)^q] = \sigma^q(q-1)!! = (\sqrt{t_j - t_{j-1}})^p(p-1)!!$ where $!!$ is the double factorial. Since the expected value of the whole function is trivially $0$ if $q$ is odd, we will only consider the situation where $q$ is even,
\begin{align*}
\lim_{N \to \infty} \Sigma^N_{j=1} f(t_{j-1})(t_j - t_{j-1})^p E[(\Delta_{j-1}^j W)^q] &= \lim_{N \to \infty} \Sigma^N_{j=1} f(t_{j-1})(t_j - t_{j-1})^p (\sqrt{t_j - t_{j-1}})^q(q-1)!!\\
&= \lim_{N \to \infty} \Sigma^N_{j=1} f(t_{j-1})(t_j - t_{j-1})^p (t_j - t_{j-1})^{q/2}(q-1)!!\\
&= (q-1)!! \lim_{N \to \infty} \Sigma^N_{j=1} f(t_{j-1})(t_j - t_{j-1})^{p + q/2}
\end{align*}

By the constraints in the problem, we know that $p + q/2 > 1$. Hence, we can apply the result of Problem 9. Hence, the limit is equal to 0,
\begin{align*}
E[\lim_{N \to \infty} \Sigma^N_{j=1} f(t_{j-1})(t_j - t_{j-1})^p(\Delta_{j-1}^j W)^q] &= (q-1)!! \lim_{N \to \infty} \Sigma^N_{j=1} f(t_{j-1})(t_j - t_{j-1})^{p + q/2}\\
&= (q-1)!! (0)\\
&= 0
\end{align*}

Now we must show that the variance is equal to $0$,
\begin{align*}
\text{Var}[\lim_{N \to \infty} \Sigma^N_{j=1} f(t_{j-1})(t_j - t_{j-1})^p(\Delta_{j-1}^j W)^q] &= 2\lim_{N \to \infty} \Sigma^N_{j=1} (f(t_{j-1}))^2 (t_j - t_{j-1})^{2p} \cdot \text{Var}[(\Delta_{j-1}^j W)^q]\\
&= 2\lim_{N \to \infty} \Sigma^N_{j=1} (f(t_{j-1}))^2 (t_j - t_{j-1})^{2p} \cdot E[(\Delta_{j-1}^j W)^{2q}]\\
&= 2\lim_{N \to \infty} \Sigma^N_{j=1} (f(t_{j-1}))^2 (t_j - t_{j-1})^{2p} \cdot E[(\Delta_{j-1}^j W)^{2q}]\\
&= 2\lim_{N \to \infty} \Sigma^N_{j=1} (f(t_{j-1}))^2 (t_j - t_{j-1})^{2p} \cdot (t_j - t_{j-1})^{q}(2q-1)\\
&= 2(2q-1) \lim_{N \to \infty} \Sigma^N_{j=1} (f(t_{j-1}))^2 (t_j - t_{j-1})^{2p+q} 
\end{align*}

By the same reasoning as in Problem 10, this limit is equal to 0. Thus,
\begin{align*}
\text{Var}[\lim_{N \to \infty} \Sigma^N_{j=1} f(t_{j-1})(t_j - t_{j-1})^p(\Delta_{j-1}^j W)^q] &= 2(2q-1) \lim_{N \to \infty} \Sigma^N_{j=1} (f(t_{j-1}))^2 (t_j - t_{j-1})^{2p+q}\\
&= 0
\end{align*}

Since the mean is equal to a constant (0 in this case) and the variance is equal to 0, we can say that the mean-squared limit of this function is 0.

\begin{problem}{12}
\end{problem}

We have that,
\begin{align*}
\Sigma_{k=m}^n f_k(g_{k+1} - g_k) &= f_m(g_{m+1} - g_m) + f_{m+1}(g_{m+2} - g_{m+1}) + f_{m+2}(g_{m+3} - g_{m+2}) + \cdots\\
&= f_m(g_m+1) - f_m(g_m) + f_{m+1}(g_{m+2}) - f_{m+1}(g_{m+1}) + \cdots\\
&= - f_m(g_m) + g_{m+1}(f_m - f_{m+1}) + g_{m+2}(f_{m+1} - f_{m+2}) + \cdots + f_n(g_{n+1})\\
&= f_n(g_{n+1}) - f_m(g_m) - \left[g_{m+1}(f_m - f_{m+1}) + g_{m+2}(f_{m+1} - f_{m+2}) + \cdots + g_{n}(f_{n-1} - f_{n})\right]\\
&= f_n(g_{n+1}) - f_m(g_m) - \Sigma_{k = m+1}^{n} g_k (f_{k-1} - f_k)
\end{align*}

\begin{problem}{13}
\end{problem}

We have $F_t = W_t$ with the interval of interest being $[0 , T]$. The partial sums of the integral $\int_0^T W_tdW_t$ are given by
\begin{align*}
S_n &= \Sigma_{i=0}^{n-1} F_{t_i}(W_{t_{i+1}} - W_{t_i})\\
&= \Sigma_{i=0}^{n-1} W_{t_i} (W_{t_{i+1}} - W_{t_i})
\end{align*}

We also have that
\begin{align*}
W_{t_i}(W_{t_{i+1}} - W_{t_i}) = \frac{1}{2}W^2_{t_{i+1}} - \frac{1}{2}W^2_{t_i} - \frac{1}{2}(W_{t_{i+1}} - W_{t_i})^2
\end{align*}

Plugging this into the partial sums formula yields,
\begin{align*}
S_n &= \frac{1}{2} \Sigma_{i=0}^{n-1} W^2_{t_{i+1}} - \frac{1}{2} \Sigma_{i=0}^{n-1} W^2_{t_i} - \frac{1}{2} \Sigma_{i=0}^{n-1} (W_{t_{i+1}} - W_{t_i})^2\\
&= \frac{1}{2}W^2_{t_n} - \frac{1}{2} \Sigma_{i=0}^{n-1} (W_{t_{i+1}} - W_{t_i})^2
\end{align*}

If we let $t_n = T$, we get
\begin{align*}
S_n = \frac{1}{2}W^2_T - \frac{1}{2} \Sigma_{i=0}^{n-1}(W_{t_{i+1}} - W_{t_i})^2
\end{align*}

Using Proposition 4.11.6 and the fact that $\frac{1}{2}W^2_T$ does not depend on $n$, we have
\begin{align*}
\operatorname{ms-\lim}_{n \to \infty} S_n &= \frac{1}{2}W^2_T - \operatorname{ms-\lim}_{n \to \infty} \frac{1}{2} \Sigma_{i=0}^{n-1}(W_{t_{i+1}} - W_{t_i})^2\\
&= \frac{1}{2}W^2_T - \frac{1}{2}T
\end{align*}

Since the Ito integral is defined as the mean-squared limit of the partial sums $S_n$, we have that
\begin{align*}
\int_0^T W_tdW_t &= \operatorname{ms-\lim}_{n \to \infty} S_n\\
&= \frac{1}{2}W^2_T - \frac{1}{2}T
\end{align*}
as required.

\begin{problem}{14}
\end{problem}

We are given the Ito integral,
\begin{align*}
\int_0^T f(t)dW_t
\end{align*}

where $f(t)$ is an arbitrary bounded and continuous function. The partial sums for this integral are given by,
\begin{align*}
S_n = \Sigma_{i = 0}^{n-1} f(t_i)(W_{t_{i+1}} - W_{t_i})
\end{align*}

\end{document}